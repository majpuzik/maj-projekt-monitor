#!/usr/bin/env python3
"""
ALMQUIST Full √öS (√östavn√≠ soud) Crawler
Crawls decisions from Constitutional Court via NALUS search
NOTE: NALUS requires Selenium for full functionality - this is simplified version
"""

import sqlite3
import requests
from bs4 import BeautifulSoup
import time
from datetime import datetime
import json
import re

class FullUSoudCrawler:
    """Full crawler for √östavn√≠ soud decisions"""

    def __init__(self, db_path="/home/puzik/almquist_legal_sources.db"):
        self.db_path = db_path
        self.session = requests.Session()
        self.session.headers.update({
            'User-Agent': 'Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36'
        })
        self.pause_between_requests = 3

    def search_decisions_simple(self, decision_type='nalez', max_pages=100):
        """
        Simple search approach - iterate through decision IDs
        √öS decisions have predictable patterns like Pl. √öS 1/93, I. √öS 100/20, etc.
        """
        print(f"\nüìã Searching {decision_type} decisions (simplified approach)")
        print("Note: Full √öS crawl would require Selenium for ASP.NET forms")
        print("Using alternative: checking common decision ID patterns\n")

        decisions_found = []

        # Common √öS prefixes
        prefixes = [
            'Pl. √öS',  # Pl√©num
            'I. √öS',   # Sen√°t I
            'II. √öS',  # Sen√°t II
            'III. √öS', # Sen√°t III
            'IV. √öS',  # Sen√°t IV
        ]

        # Years to check
        years = range(1993, 2026)  # √öS existuje od 1993

        for year_full in years:
            year_short = str(year_full)[2:]  # 2024 -> 24

            print(f"\nüìÖ Checking year {year_full}...")
            year_found = 0

            for prefix in prefixes:
                # Try first 200 numbers per prefix per year
                for num in range(1, 201):
                    case_number = f"{prefix} {num}/{year_short}"

                    # Try to fetch decision page
                    # NALUS URL pattern: /Search/Decision.aspx?id=XXXXX
                    # We'd need actual IDs - this is why Selenium is better

                    # For now, create placeholders for known important decisions
                    # In production, use Selenium to navigate search form

                    if num <= 5:  # Just sample first few for demo
                        decisions_found.append({
                            'case_number': case_number,
                            'year': year_full,
                            'decision_type': decision_type,
                            'note': 'PLACEHOLDER - Needs Selenium for full implementation'
                        })
                        year_found += 1

            if year_found > 0:
                print(f"   Found {year_found} potential decisions (simplified)")

        return decisions_found

    def crawl_decision_detail_url(self, url):
        """Crawl √öS decision from direct URL"""
        try:
            response = self.session.get(url, timeout=30)
            response.raise_for_status()
            soup = BeautifulSoup(response.text, 'html.parser')

            # Extract full text
            content = soup.find('div', class_='content') or soup.find('main')

            full_text = ""
            if content:
                full_text = content.get_text(separator='\n\n', strip=True)

            if not full_text:
                body = soup.find('body')
                if body:
                    full_text = body.get_text(separator='\n\n', strip=True)

            return full_text[:500000]

        except Exception as e:
            print(f"      ‚úó Error: {e}")
            return ""

    def save_decision(self, decision_info):
        """Save √öS decision to database"""
        conn = sqlite3.connect(self.db_path)
        cursor = conn.cursor()

        case_number = decision_info['case_number']

        cursor.execute('SELECT id FROM court_decisions WHERE case_number = ?', (case_number,))
        existing = cursor.fetchone()

        if existing:
            decision_id = existing[0]
        else:
            cursor.execute('''
            INSERT INTO court_decisions (
                case_number, court_level, court_name,
                decision_type,
                legal_area,
                full_text, source_url
            ) VALUES (?, ?, ?, ?, ?, ?, ?)
            ''', (
                case_number,
                'constitutional',
                '√östavn√≠ soud',
                decision_info.get('decision_type', 'nalez'),
                'ustavni',
                decision_info.get('note', 'PLACEHOLDER'),
                'https://nalus.usoud.cz'
            ))
            decision_id = cursor.lastrowid

        conn.commit()
        conn.close()
        return decision_id

    def log_crawl(self, source, source_type, status, items_found, items_added):
        """Log crawl to history"""
        conn = sqlite3.connect(self.db_path)
        cursor = conn.cursor()

        cursor.execute('''
        INSERT INTO crawl_history (source, source_type, status, items_found, items_added)
        VALUES (?, ?, ?, ?, ?)
        ''', (source, source_type, status, items_found, items_added))

        conn.commit()
        conn.close()

    def crawl_all_usoud(self):
        """Crawl √öS decisions - SIMPLIFIED VERSION"""
        print("=" * 70)
        print("‚öñÔ∏è  ALMQUIST √öSTAVN√ç SOUD CRAWLER (SIMPLIFIED)")
        print("=" * 70)
        print("\n‚ö†Ô∏è  WARNING: Full √öS crawl requires Selenium!")
        print("This simplified version creates placeholders for structure.")
        print("For production: Install Selenium and use automated form filling.\n")

        print("üìã Recommended approach:")
        print("  1. Use Selenium WebDriver")
        print("  2. Navigate to https://nalus.usoud.cz/Search/Search.aspx")
        print("  3. Select decision types (n√°lezy, usnesen√≠)")
        print("  4. Submit search and iterate through result pages")
        print("  5. Extract decision URLs and crawl each\n")

        print("For now, creating structural placeholders...\n")

        # Search n√°lezy (findings)
        nalezy = self.search_decisions_simple('nalez', max_pages=10)

        print(f"\n‚úÖ Found {len(nalezy)} placeholder decisions")
        print("\n‚ö†Ô∏è  To implement full √öS crawler:")
        print("   pip install selenium")
        print("   See: /home/puzik/almquist_full_usoud_selenium_crawler.py (TODO)\n")

        # Save a few samples
        saved = 0
        for decision in nalezy[:20]:  # Just first 20 samples
            decision_id = self.save_decision(decision)
            saved += 1

        self.log_crawl('usoud_full_placeholder', 'court_decision', 'partial', len(nalezy), saved)

        print(f"‚úÖ Saved {saved} placeholder decisions to database")
        print("=" * 70)


def main():
    crawler = FullUSoudCrawler()
    crawler.crawl_all_usoud()


if __name__ == "__main__":
    main()
