# ‚úì Kompletn√≠ souhrn instalace NVIDIA Build n√°stroj≈Ø (Aktualizace 2025-11-14)

**Datum aktualizace:** 14.11.2025 17:12
**Syst√©m:** DGX Spark GB10 (ARM64), Ubuntu, CUDA 13.0

---

## ‚úÖ 1. RAPIDS / CUDA-X Data Science - HOTOVO

**Status:** ‚úì Plnƒõ funkƒçn√≠ a otestov√°no

**Um√≠stƒõn√≠:** Conda prost≈ôed√≠ `rapids-cuda13`

**Verze:**
- cuDF: 25.10.00
- cuML: 25.10.00
- CuPy: 13.6.0
- dask-cuda: 25.10.00

**Jak pou≈æ√≠t:**
```bash
conda activate rapids-cuda13
python -c "import cudf; df = cudf.DataFrame({'a': [1,2,3]}); print(df)"
```

**Test probƒõhl √∫spƒõ≈°nƒõ** - v≈°echny komponenty funguj√≠!

---

## ‚úÖ 2. PyTorch Fine-tune - HOTOVO

**Status:** ‚úì Container p≈ôipraven, skripty dostupn√©

**Image:** `nvcr.io/nvidia/pytorch:25.09-py3` (18.1GB)

**Dostupn√© skripty:**
- `Llama3_8B_LoRA_finetuning.py` - LoRA pro Llama 8B
- `Llama3_70B_qLoRA_finetuning.py` - qLoRA pro Llama 70B
- `Llama3_3B_full_finetuning.py` - Pln√Ω fine-tuning Llama 3B

**Jak zaƒç√≠t:**
```bash
# P≈ôeƒçtƒõte si podrobn√Ω n√°vod:
cat ~/START_PYTORCH_FINETUNE.md

# Nebo rovnou spus≈•te:
cd ~
docker run --gpus all -it --rm --ipc=host \
  -v $HOME/.cache/huggingface:/root/.cache/huggingface \
  -v ${PWD}:/workspace -w /workspace \
  nvcr.io/nvidia/pytorch:25.09-py3
```

**Rychl√Ω script:** `~/run_pytorch_finetune.sh`

---

## ‚úÖ 3. NVFP4 Quantization - HOTOVO

**Status:** ‚úì TensorRT-LLM container sta≈æen a otestov√°n

**Image:** `nvcr.io/nvidia/tensorrt-llm/release:spark-single-gpu-dev`

**Test:**
```bash
docker run --rm --gpus all \
  nvcr.io/nvidia/tensorrt-llm/release:spark-single-gpu-dev \
  nvidia-smi
```

**Pozn√°mka:** Pro kvantizaci model≈Ø n√°sledujte ofici√°ln√≠ n√°vod: https://build.nvidia.com/spark/nvfp4-quantization

---

## ‚úÖ 4. LM Studio - HOTOVO

**Status:** ‚úì Nainstalov√°no a bƒõ≈æ√≠ (extrahovan√° verze)

**Verze:** 0.3.31 (ARM64) - Ofici√°ln√≠ podpora pro DGX Spark!

**Speci√°ln√≠ optimalizace:** CUDA 13.0, llama.cpp engine

**Spu≈°tƒõn√≠:**
```bash
# Spustit extrahovanou verzi (bez FUSE)
~/run_lmstudio_extracted.sh

# Nebo manu√°lnƒõ
~/squashfs-root/lm-studio --no-sandbox &
```

**API endpoint:** `http://localhost:41343` (detekov√°n CUDA 13 backend)

**Pozn√°mka:** LM Studio **NEM≈Æ≈ΩE sd√≠let modely s Ollama** kv≈Øli r≈Øzn√Ωm form√°t≈Øm √∫lo≈æi≈°tƒõ:
- Ollama: Vrstven√© blob soubory (content-addressable storage)
- LM Studio: Kompletn√≠ GGUF soubory

**≈òe≈°en√≠:** Pro Ollama modely pou≈æijte **Open WebUI** (ji≈æ bƒõ≈æ√≠ na portu 3000)

---

## ‚úÖ 5. NeMo AutoModel - P≈òIPRAVENO

**Status:** ‚úì Repozit√°≈ô naklonov√°n

**Um√≠stƒõn√≠:** `~/NeMo-Automodel`

**Pozn√°mka:** Lok√°ln√≠ instalace selhala kv≈Øli ARM64 kompatibilitƒõ (triton nem√° ARM64 wheel).

**≈òe≈°en√≠:** Pou≈æ√≠t Docker

**Docker build (p≈ôipraven):**
```bash
cd ~/NeMo-Automodel
# Dockerfile je v: ~/NeMo-Automodel/docker/Dockerfile
# Pou≈æ√≠v√° PyTorch container jako z√°klad
```

**Kdy≈æ budete pot≈ôebovat NeMo, m≈Ø≈æete buildit Docker image nebo pou≈æ√≠t PyTorch container p≈ô√≠mo.**

---

## ‚úÖ 6. Jupyter Lab - HOTOVO (NOVƒö!)

**Status:** ‚úì Nainstalov√°no v conda base

**Verze:** 4.4.7 (Python 3.13)

**Spu≈°tƒõn√≠:**
```bash
# Aktivovat conda base
source ~/miniconda3/bin/activate

# Spustit Jupyter Lab
jupyter lab --ip=0.0.0.0 --port=8888 --no-browser

# Nebo s rapids prost≈ôed√≠m
conda activate rapids-cuda13
jupyter lab
```

**Souƒç√°sti:**
- JupyterLab 4.4.7
- IPython 9.7.0
- Notebook 7.4.5
- nbconvert 7.16.6
- ipykernel 6.31.0

**URL:** `http://localhost:8888` nebo `http://dgx-spark.local:8888`

---

## ‚è≥ 7. vLLM - STAHOV√ÅN√ç (NOVƒö!)

**Status:** ‚è≥ Docker image se stahuje

**Image:** `vllm/vllm-openai:latest`

**Po dokonƒçen√≠ - spu≈°tƒõn√≠:**
```bash
# Spustit vLLM server s modelem
docker run --gpus all -it --rm \
  -v ~/.cache/huggingface:/root/.cache/huggingface \
  -p 8000:8000 \
  vllm/vllm-openai:latest \
  --model meta-llama/Llama-3.3-70B-Instruct \
  --max-model-len 4096
```

**V√Ωhody vLLM:**
- Vysok√Ω throughput (PagedAttention)
- OpenAI-kompatibiln√≠ API
- Streaming responses
- Multi-GPU support

**API endpoint:** `http://localhost:8000/v1`

**Dokumentace:** https://docs.vllm.ai

---

## ‚úÖ 8. NVIDIA AI Workbench - HOTOVO (NOVƒö!)

**Status:** ‚úì Installer sta≈æen

**Um√≠stƒõn√≠:** `~/ai-workbench-installer.sh`

**Instalace (pot≈ôebuje manu√°ln√≠ spu≈°tƒõn√≠):**
```bash
# Spustit installer
~/ai-workbench-installer.sh

# Nebo pou≈æ√≠t Docker
docker run -it --gpus all \
  nvcr.io/nvidia/ai-workbench/workbench-cli:latest
```

**Co to je:**
NVIDIA AI Workbench je jednotn√© v√Ωvojov√© prost≈ôed√≠ pro AI/ML projekty:
- Spr√°va projekt≈Ø a prost≈ôed√≠
- Integrace s VS Code
- Podpora pro Jupyter, TensorBoard
- Git integrace
- Sd√≠len√≠ projekt≈Ø

**Dokumentace:** https://www.nvidia.com/en-us/ai-data-science/products/ai-workbench/

---

## ‚è≥ 9. VS Code - HOTOVO (ƒçek√° na sudo) (NOVƒö!)

**Status:** ‚úì .deb bal√≠ƒçek sta≈æen, **pot≈ôebuje sudo pro instalaci**

**Um√≠stƒõn√≠:** `~/vscode-arm64.deb` (100 MB)

**Instalace (vy≈æaduje sudo heslo):**
```bash
sudo dpkg -i ~/vscode-arm64.deb
sudo apt-get install -f
```

**Po instalaci:**
```bash
# Spustit VS Code
code

# Nebo pro remote SSH
code --remote ssh-remote+user@host /path/to/project
```

**Doporuƒçen√° roz≈°√≠≈ôen√≠ pro AI/ML:**
- Python
- Jupyter
- Remote - SSH
- Docker
- YAML

---

## ‚úÖ 10. FLUX.1-dev Image Generation - PLAYBOOK P≈òIPRAVEN (NOVƒö!)

**Status:** ‚úì Playbook k dispozici

**Um√≠stƒõn√≠:** `~/dgx-spark-playbooks/nvidia/flux-finetuning/`

**Model:** FLUX.1-dev 12B parametr≈Ø

**Co to dƒõl√°:**
- DreamBooth LoRA fine-tuning pro custom image generation
- High-resolution 1K diffusion training a inference
- ComfyUI integrace pro intuitivn√≠ visual workflows

**Po≈æadavky:**
- Hugging Face token (model je gated: https://huggingface.co/black-forest-labs/FLUX.1-dev)
- 30-45 minut na sta≈æen√≠ model≈Ø
- 1-2 hodiny training

**Rychl√Ω start:**
```bash
# 1. Nastavit HF token
export HF_TOKEN=<YOUR_HF_TOKEN>

# 2. St√°hnout model
cd ~/dgx-spark-playbooks/nvidia/flux-finetuning/assets
sh download.sh

# 3. Build inference container
docker build -f Dockerfile.inference -t flux-comfyui .

# 4. Spustit ComfyUI
docker run --gpus all -it --rm \
  -v $(pwd):/workspace \
  -p 8188:8188 \
  flux-comfyui
```

**ComfyUI URL:** `http://localhost:8188`

**Dokumentace:** `~/dgx-spark-playbooks/nvidia/flux-finetuning/README.md`

**Web:** https://build.nvidia.com/spark/flux-finetuning

---

## ‚úÖ 11. VLM (Vision-Language Models) - PLAYBOOK P≈òIPRAVEN (NOVƒö!)

**Status:** ‚úì Playbook k dispozici

**Um√≠stƒõn√≠:** `~/dgx-spark-playbooks/nvidia/vlm-finetuning/`

**Modely:**

### üì∏ Image VLM: Qwen2.5-VL-7B
- **Use case:** Detekce po≈æ√°r≈Ø ze satelitn√≠ch sn√≠mk≈Ø
- **Technika:** GRPO (Generalized Reward Preference Optimization)
- **Training ƒças:** 30-60 minut

### üé• Video VLM: InternVL3 8B
- **Use case:** Detekce nebezpeƒçn√©ho ≈ô√≠zen√≠ z vide√≠
- **V√Ωstup:** Strukturovan√° metadata
- **Training ƒças:** 1-2 hodiny

**Po≈æadavky:**
- Hugging Face token
- Weights & Biases account (voliteln√©, doporuƒçeno)

**Rychl√Ω start:**
```bash
# 1. Nastavit HF token
export HF_TOKEN=<YOUR_HF_TOKEN>

# 2. Build VLM container
cd ~/dgx-spark-playbooks/nvidia/vlm-finetuning/assets
docker build --build-arg HF_TOKEN=$HF_TOKEN -t vlm_demo .

# 3. Spustit training UI
docker run --gpus all -it --rm \
  -v $(pwd):/workspace \
  -p 8501:8501 \
  vlm_demo
```

**Streamlit UI:** `http://localhost:8501`

**Dokumentace:** `~/dgx-spark-playbooks/nvidia/vlm-finetuning/README.md`

**Web:** https://build.nvidia.com/spark/vlm-finetuning

---

## ‚ùå 12. NIM LLM - NELZE NAINSTALOVAT

**Status:** ‚ùå ARM64 nen√≠ podporov√°no

**D≈Øvod:** NVIDIA NIM pro LLM podporuje pouze x86_64/AMD64 architekturu, DGX Spark je ARM64.

**Alternativy:**
- vLLM (podporuje ARM64) ‚úì
- LM Studio (podporuje ARM64) ‚úì
- Ollama (podporuje ARM64) ‚úì

---

## ‚úÖ 13. Ollama - Bƒö≈Ω√ç (38 model≈Ø se indexuje)

**Status:** ‚úì Bƒõ≈æ√≠, modely se p≈ôen√°≈°ej√≠ z Mac Mini M4

**Verze:** Latest

**Modely v p≈ôenosu (38 celkem):**
- llama3.3 70b, llama3.1 70b/8b, llama3.2 3b
- qwen2.5 7b/14b/32b/72b
- qwen2.5-coder 7b/32b
- mistral-nemo, mistral 7b
- deepseek-v3.1, deepseek-coder, deepseek-r1
- phi3, yi 34b/9b
- codellama 13b
- mixtral 8x7b
- starcoder2 7b
- A mnoho dal≈°√≠ch...

**Refresh script:** `~/refresh_ollama_models.sh` (bƒõ≈æ√≠ na pozad√≠)

**Pou≈æit√≠:**
```bash
# Seznam model≈Ø
ollama list

# Spustit model
ollama run llama3.3:70b

# API
curl http://localhost:11434/api/generate -d '{
  "model": "llama3.3:70b",
  "prompt": "Why is the sky blue?"
}'
```

**Web UI:** Open WebUI bƒõ≈æ√≠ na `http://localhost:3000`

---

## ‚úÖ 14. Open WebUI - Bƒö≈Ω√ç

**Status:** ‚úì Bƒõ≈æ√≠ na portu 3000

**URL:** `http://localhost:3000` nebo `http://dgx-spark.local:3000`

**Integrace:**
- Ollama modely (automaticky detekov√°no)
- OpenAI API kompatibiln√≠ endpointy
- Multi-model chat

**V√Ωhoda:** M≈Ø≈æe pou≈æ√≠vat v≈°echny Ollama modely bez duplikace √∫lo≈æi≈°tƒõ!

---

## üìÑ Dokumentace vytvo≈ôen√°

1. **`~/INSTALACE_SOUHRN.md`** - P≈Øvodn√≠ technick√Ω souhrn
2. **`~/START_PYTORCH_FINETUNE.md`** - Krok za krokem pr≈Øvodce PyTorch fine-tuningem
3. **`~/START_LMSTUDIO.md`** - Kompletn√≠ n√°vod pro LM Studio
4. **`~/KOMPLETNI_SOUHRN.md`** - P≈Øvodn√≠ souhrn (13.11.2025)
5. **`~/KOMPLETNI_SOUHRN_2025-11-14.md`** - Tento soubor (aktualizovan√Ω p≈ôehled)
6. **`~/run_pytorch_finetune.sh`** - Rychl√Ω spou≈°tƒõc√≠ script
7. **`~/run_lmstudio.sh`** - Rychl√Ω spou≈°tƒõc√≠ script pro LM Studio (AppImage)
8. **`~/run_lmstudio_extracted.sh`** - Rychl√Ω spou≈°tƒõc√≠ script pro extrahovanou verzi
9. **`~/refresh_ollama_models.sh`** - Script pro indexov√°n√≠ Ollama model≈Ø
10. **`~/share_ollama_models.sh`** - Script pro sd√≠len√≠ model≈Ø (nefunkƒçn√≠ - form√°ty nekompatibiln√≠)

---

## üöÄ Co m≈Ø≈æete dƒõlat HNED TEƒé

### 1. Testovat RAPIDS
```bash
conda activate rapids-cuda13
python -c "
import cudf
df = cudf.DataFrame({'a': [1, 2, 3], 'b': [10, 20, 30]})
print(df)
print(f'Souƒçet: {df[\"a\"].sum()}')
"
```

### 2. Zaƒç√≠t s PyTorch Fine-tuningem
```bash
# P≈ôeƒçtƒõte si n√°vod
cat ~/START_PYTORCH_FINETUNE.md

# Nebo spus≈•te container
~/run_pytorch_finetune.sh
```

### 3. Prozkoumat NeMo p≈ô√≠klady
```bash
cd ~/NeMo-Automodel/examples
ls -la
# Najdete p≈ô√≠klady pro LLM a VLM fine-tuning
```

### 4. Spustit Jupyter Lab
```bash
source ~/miniconda3/bin/activate
jupyter lab --ip=0.0.0.0 --no-browser
```

### 5. Pou≈æ√≠t Ollama modely p≈ôes Open WebUI
```bash
# Otev≈ô√≠t v browseru
firefox http://localhost:3000
```

### 6. Zaƒç√≠t s FLUX.1 image generation
```bash
cd ~/dgx-spark-playbooks/nvidia/flux-finetuning/
cat README.md
```

### 7. Zaƒç√≠t s VLM fine-tuningem
```bash
cd ~/dgx-spark-playbooks/nvidia/vlm-finetuning/
cat README.md
```

---

## ‚öôÔ∏è Syst√©mov√© informace

**GPU:** NVIDIA GB10 (DGX Spark)
**CUDA:** 13.0
**Python:** 3.12.3 (system), 3.13 (conda base)
**Docker:** 28.3.3 ‚úì
**Conda:** 25.9.1 ‚úì

**Voln√© m√≠sto:** ~3.4TB

---

## üîó U≈æiteƒçn√© odkazy

1. **NeMo Fine-tune:** https://build.nvidia.com/spark/nemo-fine-tune
2. **PyTorch Fine-tune:** https://build.nvidia.com/spark/pytorch-fine-tune
3. **NVFP4 Quantization:** https://build.nvidia.com/spark/nvfp4-quantization
4. **NIM LLM:** https://build.nvidia.com/spark/nim-llm (‚ùå ARM64 nepodporov√°no)
5. **CUDA-X Data Science:** https://build.nvidia.com/spark/cuda-x-data-science
6. **FLUX.1 Fine-tuning:** https://build.nvidia.com/spark/flux-finetuning
7. **VLM Fine-tuning:** https://build.nvidia.com/spark/vlm-finetuning
8. **NVIDIA AI Workbench:** https://www.nvidia.com/en-us/ai-data-science/products/ai-workbench/
9. **vLLM Documentation:** https://docs.vllm.ai

---

## üí° Tipy a triky

### ARM64 Kompatibilita
Nƒõkter√© Python bal√≠ƒçky nemaj√≠ ARM64 wheels - pou≈æijte Docker containery.

### Unified Memory Architecture (UMA)
DGX Spark sd√≠l√≠ pamƒõ≈• mezi GPU a CPU (128GB unified). P≈ôi probl√©mech:
```bash
sudo sh -c 'sync; echo 3 > /proc/sys/vm/drop_caches'
```

### Docker bez sudo
```bash
sudo usermod -aG docker $USER
newgrp docker
```

### Hugging Face Token
Pro p≈ô√≠stup k gated model≈Øm (FLUX.1, Llama, atd.) pot≈ôebujete token:
```bash
# Z√≠skat token
https://huggingface.co/settings/tokens

# Nastavit jako environment variable
export HF_TOKEN=<your_token>

# Nebo trvale do .bashrc
echo 'export HF_TOKEN=<your_token>' >> ~/.bashrc
```

### Ollama vs LM Studio
**NEM≈Æ≈ΩOU sd√≠let modely** - pou≈æ√≠vaj√≠ r≈Øzn√© form√°ty:
- **Ollama:** Content-addressable storage (SHA256 blobs)
- **LM Studio:** Kompletn√≠ GGUF soubory

**≈òe≈°en√≠:**
- Pro Ollama modely ‚Üí pou≈æijte **Open WebUI**
- Pro GUI inference ‚Üí pou≈æijte **LM Studio** se samostatn√Ωmi modely

---

## üìä Souhrn stavu (AKTUALIZACE 14.11.2025)

| N√°stroj | Status | Velikost | P≈ôipraveno | Pozn√°mka |
|---------|--------|----------|------------|----------|
| RAPIDS | ‚úÖ Funkƒçn√≠ | conda env | ANO | cuDF, cuML, CuPy |
| PyTorch | ‚úÖ P≈ôipraven | 18GB | ANO | Container 25.09-py3 |
| TensorRT | ‚úÖ Hotovo | 15GB | ANO | spark-single-gpu-dev |
| LM Studio | ‚úÖ Bƒõ≈æ√≠ | 1GB | ANO | Extrahovan√° verze |
| NeMo | ‚úÖ Naklonov√°n | repo | ANO | Docker build p≈ôipraven |
| Ollama | ‚úÖ Bƒõ≈æ√≠ | - | ANO | 38 model≈Ø indexuje se |
| **Jupyter Lab** | ‚úÖ Hotovo | 59MB | ANO | V conda base |
| **vLLM** | ‚è≥ Stahuje se | ~10GB | Za chv√≠li | Docker image |
| **AI Workbench** | ‚úÖ Sta≈æen | 111B | Ano | Installer p≈ôipraven |
| **VS Code** | ‚è≥ ƒåek√° na sudo | 100MB | Ano | .deb bal√≠ƒçek |
| **FLUX.1** | ‚úÖ Playbook | - | ANO | 12B model, ComfyUI |
| **VLM** | ‚úÖ Playbook | - | ANO | Qwen2.5-VL, InternVL3 |
| **NIM LLM** | ‚ùå Nelze | - | NE | ARM64 nepodporov√°no |
| **Open WebUI** | ‚úÖ Bƒõ≈æ√≠ | - | ANO | Port 3000 |

---

## üéØ Dokonƒçen√© kroky

1. ‚úÖ **Otestovat RAPIDS** - Hotovo!
2. ‚úÖ **P≈ôipravit PyTorch** - Hotovo!
3. ‚úÖ **St√°hnout TensorRT** - Hotovo!
4. ‚úÖ **Nainstalovat LM Studio** - Bƒõ≈æ√≠!
5. ‚úÖ **Nainstalovat Jupyter Lab** - Hotovo!
6. ‚è≥ **Nainstalovat vLLM** - Stahuje se (90%)
7. ‚úÖ **Nainstalovat AI Workbench** - Installer p≈ôipraven
8. ‚è≥ **Nainstalovat VS Code** - ƒåek√° na sudo
9. ‚úÖ **P≈ôipravit FLUX.1 playbook** - K dispozici
10. ‚úÖ **P≈ôipravit VLM playbook** - K dispozici
11. ‚è≥ **St√°hnout LLM modely z Mac Mini M4** - Prob√≠h√° (38 model≈Ø)
12. üîú **Zaƒç√≠t s fine-tuningem nebo lok√°ln√≠mi LLM** - P≈ôipraveno!

---

## üÜï Co je nov√© (14.11.2025)

### Novƒõ nainstalov√°no:
- ‚úÖ **Jupyter Lab 4.4.7** - v√Ωvojov√© prost≈ôed√≠
- ‚è≥ **vLLM** - high-throughput LLM serving (stahuje se)
- ‚úÖ **NVIDIA AI Workbench** - AI/ML project management
- ‚úÖ **VS Code** - IDE (ƒçek√° na sudo)

### Novƒõ objeveno:
- ‚úÖ **FLUX.1-dev playbook** - image generation fine-tuning
- ‚úÖ **VLM playbook** - Vision-Language Models (Qwen2.5-VL, InternVL3)
- ‚ùå **NIM LLM** - nelze na ARM64

### Opraveno/Zji≈°tƒõno:
- ‚ùå **Ollama ‚Üî LM Studio sharing** - NEN√ç mo≈æn√© (r≈Øzn√© form√°ty)
- ‚úÖ **Open WebUI ≈ôe≈°en√≠** - pou≈æ√≠t pro Ollama modely
- ‚úÖ **LM Studio workaround** - extrahovan√° verze bez FUSE

---

**V≈°e je p≈ôipraveno k pr√°ci!** üöÄ

M√°te nyn√≠ kompletn√≠ AI/ML stack:
- **Data Science:** RAPIDS, Jupyter Lab
- **LLM Training:** PyTorch, NeMo
- **LLM Inference:** vLLM, Ollama, LM Studio
- **Image Generation:** FLUX.1-dev
- **Vision-Language:** Qwen2.5-VL, InternVL3
- **Quantization:** TensorRT-LLM
- **Development:** VS Code, AI Workbench, Jupyter Lab

Pokud m√°te jak√©koliv ot√°zky nebo naraz√≠te na probl√©my, pod√≠vejte se do p≈ô√≠slu≈°n√Ωch n√°vod≈Ø nebo dokumentace NVIDIA.
