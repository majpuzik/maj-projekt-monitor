# ğŸ§¹ ALMQUIST DEDUPLICATION GUIDE

## ğŸ“‹ PÅ˜EHLED

KompletnÃ­ systÃ©m pro prevenci a odstranÄ›nÃ­ duplicitnÃ­ch dokumentÅ¯ v RAG systÃ©mu.

---

## ğŸ” ANALÃZA SOUÄŒASNÃ‰HO STAVU

### DatabÃ¡ze Statistiky (2025-11-30)

| Typ | Total | Unique | Duplicity | Metoda |
|-----|-------|--------|-----------|--------|
| **ZÃ¡kony** | 1,038 | 1,038 | 0 | âœ… case_number |
| **SoudnÃ­ rozhodnutÃ­ (ID)** | 94,260 | 92,115 | 2,145 | âš ï¸ case_number |
| **SoudnÃ­ rozhodnutÃ­ (Content)** | 94,260 | 94,212 | 48 | âœ… SHA256 hash |

### KlÃ­ÄovÃ¡ zjiÅ¡tÄ›nÃ­:

1. **Å½Ã¡dnÃ© duplicity v zÃ¡konech** âœ…
2. **2,145 "duplicit" podle case_number** - Äasto rÅ¯znÃ© verze tÃ©hoÅ¾ rozhodnutÃ­ (opravy, aktualizace)
3. **Jen 48 skuteÄnÃ½ch duplicit** - identickÃ½ obsah (content hash)

---

## ğŸ›¡ï¸ STRATEGIE DEDUPLIKACE

### 1. Prevence (Merger s Content Hash)

**Upgraded Merger** `/home/puzik/almquist_rag_merger.py`:
- âœ… Kontroluje document ID (law_number, case_number)
- âœ… Kontroluje SHA256 content hash
- âœ… UklÃ¡dÃ¡ content_hash do RAG metadata
- âœ… Blokuje duplicity v rÃ¡mci jednoho batch merge

**PÅ™Ã­klad:**
```python
# StarÃ¡ verze (jen ID)
if doc_id not in existing_ids:
    add_document()

# NovÃ¡ verze (ID + content hash)
if doc_id not in existing_ids and content_hash not in existing_hashes:
    add_document()
```

### 2. Detekce (Deduplication Tool)

**NÃ¡stroj:** `/home/puzik/almquist_deduplication_tool.py`

**Funkce:**
- Analyzuje databÃ¡zi na duplicity (ID i content hash)
- Detekuje duplicity v RAG metadata
- VyÄistÃ­ databÃ¡zi (ponechÃ¡ nejnovÄ›jÅ¡Ã­ verzi)
- Vacuum databÃ¡ze pro uvolnÄ›nÃ­ mÃ­sta
- AutomatickÃ© zÃ¡lohy pÅ™ed Ãºpravami

---

## ğŸš€ POUÅ½ITÃ

### 1. AnalÃ½za Duplicit (Read-Only)

```bash
# RychlÃ¡ analÃ½za databÃ¡ze
python3 /home/puzik/almquist_deduplication_tool.py --analyze
```

**Output:**
```
ğŸ“‹ LAWS: 1038 total, 1038 unique, 0 duplicates
âš–ï¸  COURT DECISIONS: 94260 total, 92115 unique, 2145 duplicates
ğŸ” CONTENT HASH: 94212 unique, 47 groups, 48 duplicates
```

### 2. AnalÃ½za RAG Duplicit

```bash
# Zkontrolovat RAG metadata na duplicity
python3 /home/puzik/almquist_deduplication_tool.py --analyze-rag
```

### 3. VyÄiÅ¡tÄ›nÃ­ DatabÃ¡ze (Dry-Run)

```bash
# UkÃ¡zat co by bylo vymazÃ¡no (bez zmÄ›n)
python3 /home/puzik/almquist_deduplication_tool.py --deduplicate-db --dry-run
```

### 4. SkuteÄnÃ© VyÄiÅ¡tÄ›nÃ­ DatabÃ¡ze

```bash
# VAROVÃNÃ: MÄ›nÃ­ databÃ¡zi! (vytvÃ¡Å™Ã­ zÃ¡lohu)
python3 /home/puzik/almquist_deduplication_tool.py --deduplicate-db
```

**Proces:**
1. âœ… VytvoÅ™Ã­ zÃ¡lohu databÃ¡ze
2. ğŸ” Najde duplicity podle case_number (ponechÃ¡ nejnovÄ›jÅ¡Ã­)
3. ğŸ” Najde duplicity podle content hash (ponechÃ¡ nejnovÄ›jÅ¡Ã­)
4. ğŸ—‘ï¸ SmaÅ¾e duplicitnÃ­ zÃ¡znamy
5. âœ… Commit zmÄ›n

### 5. Full Cleanup (All-in-One)

```bash
# KompletnÃ­ vyÄiÅ¡tÄ›nÃ­: analyze â†’ deduplicate â†’ vacuum â†’ re-analyze
python3 /home/puzik/almquist_deduplication_tool.py --full-cleanup
```

**Kroky:**
1. AnalÃ½za PÅ˜ED
2. Deduplikace
3. Vacuum (uvolnÄ›nÃ­ mÃ­sta)
4. AnalÃ½za PO (ovÄ›Å™enÃ­)

### 6. Vacuum DatabÃ¡ze

```bash
# Uvolnit mÃ­sto po smazÃ¡nÃ­ zÃ¡znamÅ¯
python3 /home/puzik/almquist_deduplication_tool.py --vacuum
```

---

## ğŸ”„ AUTOMATIZACE

### Cron Job pro PravidelnÃ© ÄŒiÅ¡tÄ›nÃ­

**MoÅ¾nost 1: MÄ›sÃ­ÄnÃ­ vyÄiÅ¡tÄ›nÃ­**

```bash
# PÅ™idat do crontabu
crontab -e
```

```cron
# Full cleanup prvnÃ­ den v mÄ›sÃ­ci ve 3:00
0 3 1 * * /home/puzik/miniconda3/bin/python3 /home/puzik/almquist_deduplication_tool.py --full-cleanup >> /home/puzik/logs/deduplication.log 2>&1
```

**MoÅ¾nost 2: TÃ½dennÃ­ analÃ½za**

```cron
# AnalÃ½za kaÅ¾dÃ© pondÄ›lÃ­ v 1:00
0 1 * * 1 /home/puzik/miniconda3/bin/python3 /home/puzik/almquist_deduplication_tool.py --analyze >> /home/puzik/logs/deduplication_analysis.log 2>&1
```

---

## ğŸ“Š OÄŒEKÃVANÃ‰ VÃSLEDKY

### PÅ™ed DeduplikacÃ­:

```
Database size: ~45 MB
Court decisions: 94,260 records
Duplicates: 2,145 (by ID), 48 (by content)
```

### Po Deduplikaci:

```
Database size: ~44 MB (Ãºspora ~1 MB)
Court decisions: 94,212 records
Duplicates: 0
```

### RAG Impact:

Merger s content hash **automaticky blokuje** duplicity:
- âœ… NovÃ© duplicity se **nepÅ™idajÃ­** do RAG
- âœ… ExistujÃ­cÃ­ RAG zÅ¯stÃ¡vÃ¡ ÄistÃ½
- âœ… Ãšspora storage a compute

---

## ğŸ›¡ï¸ BEZPEÄŒNOST

### AutomatickÃ© ZÃ¡lohy:

**Database Backup:**
```
/home/puzik/almquist_rag_backups/
â””â”€â”€ legal_db_backup_20251130_174500.db
```

**RAG Backup (pÅ™ed merge):**
```
/home/puzik/almquist_rag_backups/
â””â”€â”€ legal_rag_backup_20251130_180000/
    â”œâ”€â”€ embeddings.npy
    â”œâ”€â”€ faiss_index.bin
    â””â”€â”€ metadata.json
```

### Restore z Backup:

```bash
# Obnovit databÃ¡zi
cp /home/puzik/almquist_rag_backups/legal_db_backup_*.db \
   /home/puzik/almquist_legal_sources.db

# Obnovit RAG
cp -r /home/puzik/almquist_rag_backups/legal_rag_backup_*/* \
      /home/puzik/almquist_legal_rag/
```

---

## ğŸ”¬ DETEKCE METODY

### 1. Document ID Check (Fast)

```python
existing_ids = set()
for meta in existing_metadata:
    if meta['document_type'] == 'law':
        existing_ids.add(f"law_{meta['law_number']}")
    elif meta['document_type'] == 'court_decision':
        existing_ids.add(f"case_{meta['case_number']}")

if doc_id not in existing_ids:
    # Not a duplicate by ID
```

**VÃ½hody:**
- âš¡ Velmi rychlÃ© (O(1) lookup)
- ğŸ¯ Detekuje duplicitnÃ­ case_number

**NevÃ½hody:**
- âš ï¸ MÅ¯Å¾e oznaÄit rÅ¯znÃ© verze jako duplicity
- âš ï¸ Nedetekuje obsah-duplicity s rÅ¯znÃ½mi ID

### 2. Content Hash Check (Precise)

```python
def compute_content_hash(text: str) -> str:
    return hashlib.sha256(text.encode('utf-8')).hexdigest()

existing_hashes = set()
for doc in documents:
    content_hash = compute_content_hash(doc['text'])
    if content_hash not in existing_hashes:
        # Not a duplicate by content
        existing_hashes.add(content_hash)
```

**VÃ½hody:**
- âœ… 100% pÅ™esnÃ© (identickÃ½ obsah = duplicita)
- âœ… Detekuje rÅ¯znÃ¡ ID ale stejnÃ½ obsah
- âœ… UmoÅ¾Åˆuje rÅ¯znÃ© verze tÃ©hoÅ¾ rozhodnutÃ­

**NevÃ½hody:**
- ğŸŒ PomalejÅ¡Ã­ (musÃ­ hashovat celÃ½ text)
- ğŸ’¾ VyÅ¡Å¡Ã­ pamÄ›Å¥ovÃ¡ nÃ¡roÄnost

### 3. KombinovanÃ½ PÅ™Ã­stup (DoporuÄeno)

```python
# Merger pouÅ¾Ã­vÃ¡ OBA
if doc_id not in existing_ids and content_hash not in existing_hashes:
    add_document()
    existing_ids.add(doc_id)
    existing_hashes.add(content_hash)
```

**VÃ½hody:**
- âš¡ RychlÃ© (ID check je O(1))
- âœ… PÅ™esnÃ© (content hash je 100%)
- ğŸ›¡ï¸ DvojitÃ¡ ochrana

---

## ğŸ“ˆ METRIKY & MONITORING

### Logy:

```bash
# Sledovat deduplikaÄnÃ­ logy
tail -f /home/puzik/logs/deduplication.log

# Sledovat merge logy (content hash v akci)
tail -f /home/puzik/logs/almquist_rag_merge.log
```

### Statistiky:

```bash
# PoÄet zÃ¡znamÅ¯ v databÃ¡zi
sqlite3 /home/puzik/almquist_legal_sources.db \
  "SELECT COUNT(*) FROM court_decisions WHERE full_text IS NOT NULL"

# PoÄet unique case_numbers
sqlite3 /home/puzik/almquist_legal_sources.db \
  "SELECT COUNT(DISTINCT case_number) FROM court_decisions WHERE full_text IS NOT NULL"

# Velikost databÃ¡ze
ls -lh /home/puzik/almquist_legal_sources.db
```

---

## ğŸ¯ BEST PRACTICES

### 1. PravidelnÃ¡ AnalÃ½za

```bash
# TÃ½dennÃ­ check
python3 /home/puzik/almquist_deduplication_tool.py --analyze
```

### 2. MÄ›sÃ­ÄnÃ­ Cleanup

```bash
# PrvnÃ­ den v mÄ›sÃ­ci
python3 /home/puzik/almquist_deduplication_tool.py --full-cleanup
```

### 3. PÅ™ed VelkÃ½m Merge

```bash
# VyÄistit PÅ˜ED merge velkÃ©ho mnoÅ¾stvÃ­ dat
python3 /home/puzik/almquist_deduplication_tool.py --deduplicate-db
python3 /home/puzik/almquist_rag_merger.py
```

### 4. Po ZmÄ›nÄ› CrawlerÅ¯

```bash
# KdyÅ¾ se zmÄ›nÃ­ crawler logika
python3 /home/puzik/almquist_deduplication_tool.py --analyze
```

---

## ğŸ”§ TROUBLESHOOTING

### ProblÃ©m: "Too many duplicates"

**Å˜eÅ¡enÃ­:**
```bash
# 1. Analyzovat
python3 /home/puzik/almquist_deduplication_tool.py --analyze

# 2. VyÄistit databÃ¡zi
python3 /home/puzik/almquist_deduplication_tool.py --full-cleanup

# 3. Rebuild RAG (pokud potÅ™eba)
# RAG merger automaticky vynechÃ¡ duplicity s content hash
```

### ProblÃ©m: "Database growing too fast"

**PÅ™Ã­Äina:** Crawlery uklÃ¡dajÃ­ duplicity

**Å˜eÅ¡enÃ­:**
```bash
# 1. Vypnout crawlery doÄasnÄ›
# 2. VyÄistit databÃ¡zi
python3 /home/puzik/almquist_deduplication_tool.py --full-cleanup

# 3. Zkontrolovat crawler logiku
# 4. Restart crawlerÅ¯
```

### ProblÃ©m: "RAG queries returning duplicate results"

**Kontrola:**
```bash
# Analyzovat RAG metadata
python3 /home/puzik/almquist_deduplication_tool.py --analyze-rag
```

**Å˜eÅ¡enÃ­:** RAG rebuild (pokud jsou duplicity v metadata)

---

## ğŸ“š SOUBORY

| Soubor | Popis |
|--------|-------|
| `almquist_deduplication_tool.py` | DeduplikaÄnÃ­ nÃ¡stroj |
| `almquist_rag_merger.py` | Merger s content hash prevencÃ­ |
| `almquist_legal_sources.db` | DatabÃ¡ze (mÅ¯Å¾e obsahovat duplicity) |
| `almquist_legal_rag/metadata.json` | RAG metadata (s content_hash) |

---

## âœ… ZÃVÄšR

**SystÃ©m deduplikace je kompletnÃ­:**

1. âœ… **Prevence** - Merger blokuje duplicity pomocÃ­ content hash
2. âœ… **Detekce** - Deduplication tool analyzuje databÃ¡zi i RAG
3. âœ… **OdstranÄ›nÃ­** - AutomatickÃ© vyÄiÅ¡tÄ›nÃ­ s backup
4. âœ… **Monitoring** - Logy a statistiky
5. âœ… **Automatizace** - Cron jobs pro pravidelnÃ© ÄiÅ¡tÄ›nÃ­

**DoporuÄenÃ­:**
- Spustit `--full-cleanup` jednou mÄ›sÃ­ÄnÄ›
- Sledovat logy pravidelnÄ›
- Nechat merger s content hash prevencÃ­ aktivnÃ­ (jiÅ¾ je)

---

*Dokument vytvoÅ™en: 2025-11-30*
*Status: âœ… Production Ready*
