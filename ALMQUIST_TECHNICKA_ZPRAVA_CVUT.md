# ALMQUIST: ModernÃ­ Open Source DialogovÃ½ SystÃ©m

**TechnickÃ¡ zprÃ¡va o vÃ½voji**

---

**Autor:** M.A.J. Puzik (jedinÃ½ vÃ½vojÃ¡Å™)
**SpoleÄnost:** Software Consulting s.r.o.
**Development tool:** Claude CLI (Anthropic Sonnet 4) - AI-assisted development

**Projekt:** ALMQUIST - Open Source Conversational AI System
**Inspirace:** ALQUIST Framework (FEE ÄŒVUT)
**ObdobÃ­ vÃ½voje:** 11. - 25. listopadu 2025 (14 dnÃ­)
**Status:** ExperimentÃ¡lnÃ­ vÃ½voj - FÃ¡ze 3 (Dataset & Fine-tuning)

---

**Metodologie vÃ½voje:**
CelÃ½ projekt byl vytvoÅ™en vÃ½luÄnÄ› pomocÃ­ **Claude CLI** (Command Line Interface) od spoleÄnosti Anthropic, verze **Claude Sonnet 4**. VÅ¡echny komponenty - od architektury, pÅ™es implementaci kÃ³du, dataset generation, aÅ¾ po dokumentaci - byly vyvinuty v kolaboraci s AI asistentem v pÅ™Ã­kazovÃ© Å™Ã¡dce. Tento pÅ™Ã­stup umoÅ¾nil dosÃ¡hnout vysokÃ© produktivity (~2,700 Å™Ã¡dkÅ¯ kÃ³du/den) pÅ™i zachovÃ¡nÃ­ kvality kÃ³du a komplexnosti systÃ©mu.

---

## ABSTRAKT

Tento dokument popisuje vÃ½voj systÃ©mu ALMQUIST, modernÃ­ open source implementace konverzaÄnÃ­ho AI systÃ©mu inspirovanÃ©ho frameworkem ALQUIST vyvinutÃ½m na FakultÄ› elektrotechnickÃ© ÄŒVUT. ALMQUIST pÅ™edstavuje evoluci od rule-based dialogovÃ½ch systÃ©mÅ¯ k architektuÅ™e zaloÅ¾enÃ© na velkÃ½ch jazykovÃ½ch modelech (LLM) s integracÃ­ RAG (Retrieval-Augmented Generation) a dÅ¯razem na empatickou komunikaci v ÄeskÃ©m jazyce. SystÃ©m kombinuje nejnovÄ›jÅ¡Ã­ technologie strojovÃ©ho uÄenÃ­ s vÃ½zkumnÃ½mi poznatky z ÃºspÄ›Å¡nÃ© ÃºÄasti ALQUIST systÃ©mu v soutÄ›Å¾i Amazon Alexa Prize SocialBot Grand Challenge.

**KlÃ­ÄovÃ¡ slova:** KonverzaÄnÃ­ AI, Large Language Models, RAG, Fine-tuning, Empathetic Dialogue, Czech NLP

---

## OBSAH

1. [Ãšvod a motivace](#1-Ãºvod-a-motivace)
2. [AnalÃ½za ALQUIST frameworku](#2-analÃ½za-alquist-frameworku)
3. [Architektura ALMQUIST](#3-architektura-almquist)
4. [Implementace](#4-implementace)
5. [Dataset a training pipeline](#5-dataset-a-training-pipeline)
6. [TestovÃ¡nÃ­ a vyhodnocenÃ­](#6-testovÃ¡nÃ­-a-vyhodnocenÃ­)
7. [PorovnÃ¡nÃ­ ALQUIST vs ALMQUIST](#7-porovnÃ¡nÃ­-alquist-vs-almquist)
8. [Diskuse a budoucÃ­ prÃ¡ce](#8-diskuse-a-budoucÃ­-prÃ¡ce)
9. [ZÃ¡vÄ›r](#9-zÃ¡vÄ›r)
10. [Reference](#10-reference)

---

## 1. ÃšVOD A MOTIVACE

### 1.1 Kontext projektu

DialogovÃ© systÃ©my (socialbots) jsou sloÅ¾itÃ© systÃ©my schopnÃ© vÃ©st pÅ™irozenou konverzaci s uÅ¾ivateli napÅ™Ã­Ä rÅ¯znÃ½mi domÃ©nami. V roce 2024 doÅ¡lo k vÃ½raznÃ©mu posunu v tÃ©to oblasti dÃ­ky nÃ¡stupu velkÃ½ch jazykovÃ½ch modelÅ¯ (LLM), kterÃ© umoÅ¾nily pÅ™echod od deterministickÃ½ch rule-based systÃ©mÅ¯ k generativnÃ­m pÅ™Ã­stupÅ¯m.

SystÃ©m ALQUIST, vyvinutÃ½ na FEE ÄŒVUT, se ÃºspÄ›Å¡nÄ› ÃºÄastnil soutÄ›Å¾e Amazon Alexa Prize SocialBot Grand Challenge, kde dosÃ¡hl vÃ½raznÃ½ch ÃºspÄ›chÅ¯:
- **SGC4 (2021)**: 1. mÃ­sto - vÃ­tÄ›z soutÄ›Å¾e
- **SGC5 (2023)**: Top 5 globÃ¡lnÄ›, Top 2 v multimodÃ¡lnÃ­ch systÃ©mech, #1 v EvropÄ›

### 1.2 Motivace pro ALMQUIST

HlavnÃ­ motivace pro vÃ½voj ALMQUIST:

1. **Modernizace paradigmatu**: PÅ™echod od rule-based state machines k LLM-based architektuÅ™e
2. **Open source pÅ™Ã­stup**: PlnÄ› transparentnÃ­ systÃ©m dostupnÃ½ pro akademickou obec
3. **Czech-first design**: Optimalizace pro ÄeskÃ½ jazyk jako primÃ¡rnÃ­ cÃ­l
4. **Empatie a personalita**: DÅ¯raz na empatickou komunikaci mÃ­sto pouhÃ© fakticity
5. **Integrace RAG**: Kombinace generativnÃ­ch modelÅ¯ s faktickÃ½mi znalostmi

### 1.3 CÃ­le projektu

**HlavnÃ­ cÃ­le:**
- VytvoÅ™it modernÃ­ konverzaÄnÃ­ systÃ©m zaloÅ¾enÃ½ na LLM
- Integrovat RAG pro pÅ™esnÃ© odpovÄ›di zaloÅ¾enÃ© na znalostech
- Implementovat fine-tuning pro empatickÃ½ komunikaÄnÃ­ styl
- Zachovat kompatibilitu s vÃ½zkumnÃ½mi poznatky z ALQUIST projektÅ¯
- PÅ™ipravit systÃ©m pro ÃºÄast v mezinÃ¡rodnÃ­ch soutÄ›Å¾Ã­ch (CPDC 2025, Alexa Prize SGC6)

**SekundÃ¡rnÃ­ cÃ­le:**
- SystematickÃ© logovÃ¡nÃ­ a analÃ½za konverzacÃ­
- AutomatizovanÃ¡ generace trÃ©novacÃ­ch dat z RAG
- Podpora pro multimodÃ¡lnÃ­ rozÅ¡Ã­Å™enÃ­ (voice, 3D avatar)
- Dokumentace vhodnÃ¡ pro akademickÃ© vyuÅ¾itÃ­

---

## 2. ANALÃZA ALQUIST FRAMEWORKU

### 2.1 ALQUIST 5.0 - PÅ™ehled

ALQUIST 5.0 (arXiv:2310.16119v2) pÅ™edstavuje hybridnÃ­ architekuru kombinujÃ­cÃ­ dialogue trees s generativnÃ­mi modely.

**KlÃ­ÄovÃ© komponenty:**
- **NRG Barista**: ModifikovanÃ½ BlenderBot 3, 15-192Ã— rychlejÅ¡Ã­ neÅ¾ originÃ¡l
- **VicuChat**: Vicuna 7B s LoRA adaptÃ©ry (92% pouÅ¾itÃ­)
- **APIHub**: Integrace Evi, DuckDuckGo, Wikipedia, News API
- **3D Persona**: MetaHuman "Alquistyna" v Unreal Engine 5
- **Safety System**: Kombinace klasifikÃ¡torÅ¯ + pravidel (F1 score 0.901)

**VÃ½sledky v SGC5:**
- Rating: 3.4/5.0 (cÃ­l: 4.0)
- PrÅ¯mÄ›rnÃ¡ dÃ©lka: 15:41 min pÅ™i 90. percentilu (cÃ­l: 20+ min)
- Latence: 2.2s prÅ¯mÄ›rnÄ›

### 2.2 IdentifikovanÃ© slabiny ALQUIST 5.0

Z oficiÃ¡lnÃ­ publikace (Kobza et al., 2024):

1. **Halucinace**: NezmÄ›nÄ›no od BlenderBot 3
2. **VypnutÃ¡ dlouhodobÃ¡ pamÄ›Å¥**: Performance dÅ¯vody
3. **VysokÃ¡ latence**: 2.2s prÅ¯mÄ›r
4. **MÄ›lkÃ© konverzace**: NedostateÄnÃ¡ hloubka
5. **KrÃ¡tkÃ© odpovÄ›di**: PÅ™Ã­liÅ¡ struÄnÃ© vÃ½stupy
6. **NerelevantnÃ­ odpovÄ›di**: ObÄasnÃ¡ ztrÃ¡ta kontextu
7. **RychlÃ© zmÄ›ny tÃ©mat**: NedostateÄnÃ¡ persistence
8. **NÃ­zkÃ¡ proaktivita**: SystÃ©m mÃ¡lo iniciuje tÃ©mata
9. **Repetice**: ÄŒÃ¡steÄnÄ› vyÅ™eÅ¡eno, ale stÃ¡le pÅ™Ã­tomno
10. **Underutilized multimodal**: 3D avatar nedostateÄnÄ› vyuÅ¾it

### 2.3 ArchitektonickÃ© poznatky

**ALQUIST paradigma:**
```
Rule-based State Machine + LLM Fallback
â”œâ”€â”€ YAML definovanÃ© dialogue trees
â”œâ”€â”€ ExplicitnÃ­ state transitions
â”œâ”€â”€ Template-based responses
â””â”€â”€ LLM pouze pro unknown states
```

**VÃ½hody:**
- âœ… DeterministickÃ© chovÃ¡nÃ­
- âœ… SnadnÃ½ debugging
- âœ… Compliance-ready
- âœ… RychlÃ© odpovÄ›di

**NevÃ½hody:**
- âŒ RigidnÃ­ flow
- âŒ VysokÃ½ maintenance overhead
- âŒ OmezenÃ¡ flexibilita
- âŒ Scaling problÃ©my pÅ™i rÅ¯stu domÃ©n

---

## 3. ARCHITEKTURA ALMQUIST

### 3.1 Paradigma shift

ALMQUIST volÃ­ **LLM-first pÅ™Ã­stup** s RAG integracÃ­:

```
USER INPUT
    â†“
DIALOGOVÃ MANAÅ½ER
â”œâ”€ Context Loading
â”œâ”€ Situation Classification
â””â”€ Decision Router
    â†“
    â”œâ”€â†’ RAG Engine (Knowledge queries)
    â”œâ”€â†’ Pure LLM (Smalltalk, emotion)
    â””â”€â†’ Scenarios (Structured flows)
    â†“
STYLISTIC LAYER
â”œâ”€ Empathy injection
â”œâ”€ Humor calibration
â””â”€ Personality consistency
    â†“
RESPONSE
```

### 3.2 Komponenty systÃ©mu

#### 3.2.1 LLM Backend

**Base model:** Qwen2.5-7B-Instruct

**DÅ¯vody volby:**
- ModernÃ­ architektura (2025)
- SilnÃ¡ performance v benchmarcÃ­ch
- DobrÃ© Czech language capabilities
- EfektivnÃ­ kvantizace (4-bit moÅ¾nÃ¡)
- Open source (Apache 2.0)

**Fine-tuning pÅ™Ã­stup:**
- **Metoda**: QLoRA (4-bit quantization)
- **Framework**: Unsloth (3Ã— rychlejÅ¡Ã­ neÅ¾ HuggingFace)
- **Hardware**: DGX SPART GB10 (Software Consulting s.r.o.) / Mac M4 (MLX) / RunPod
- **Dataset size**: 38,026 seed examples â†’ cÃ­l 500-1000 high-quality

#### 3.2.2 RAG Engine

**Architektura:**
```
Query â†’ Embedding â†’ Qdrant Search â†’ Context Injection â†’ LLM
```

**Specifikace:**
- **Vector DB**: Qdrant (open source)
- **Embedding model**: nomic-embed-text (Ollama)
- **Vector dimensions**: 384 (all-MiniLM-L6-v2 compatible)
- **Distance metric**: Cosine similarity
- **Knowledge sources**:
  - ALQUIST papers (2.0, 4.0, 5.0)
  - GitHub dokumentace
  - Conversation logs
  - VlastnÃ­ knowledge base

**IndexovanÃ© dokumenty (k 25.11.2025):**
- `almqist_conversations`: 38,026 conversation seeds
- `almqist_cdb`: 72 events z centrÃ¡lnÃ­ databÃ¡ze
- `almqist_knowledge`: Papers a dokumentace

#### 3.2.3 DialogovÃ½ manaÅ¾er

**Komponenty:**

1. **Context Manager**
   - Redis pro short-term memory (session-based)
   - PostgreSQL pro long-term history
   - Context snapshots (Alquist-inspired undo mechanism)

2. **Situation Classifier**
   - Detekce typu dotazu: technical / emotional / smalltalk
   - Decision: kdy pouÅ¾Ã­t RAG vs pure LLM
   - Confidence scoring

3. **Decision Engine**
   - Routing logic (RAG / scenario / LLM)
   - Threshold management
   - Fallback strategies

**ImplementaÄnÃ­ status:**
- âŒ PlÃ¡novÃ¡no, zatÃ­m neimplementovÃ¡no
- ğŸ“Š Design dokument existuje (ALMQUIST_ARCHITECTURE_ANALYSIS_AND_RECOMMENDATIONS.md)

#### 3.2.4 Stylistic Layer

**CÃ­l:** KonzistentnÃ­ osobnost napÅ™Ã­Ä vÅ¡emi odpovÄ›Ämi

**Charakteristiky:**
- **TÃ³n**: PÅ™Ã¡telskÃ½, ne faleÅ¡nÄ› nadÅ¡enÃ½
- **Empatie**: SkuteÄnÃ¡ validace pocitÅ¯
- **Humor**: JemnÃ½, vÄ›cnÃ½, bez trapnÃ½ch vtÃ­pkÅ¯
- **FormÃ¡lnost**: NeformÃ¡lnÃ­ (tykÃ¡nÃ­)
- **Transparence**: "NevÃ­m" je pÅ™ijatelnÃ¡ odpovÄ›Ä

**Implementace:**
- System prompt engineering
- Post-processing filters (remove robotic patterns)
- Fine-tuning na empatickÃ½ch dialozÃ­ch

### 3.3 SrovnÃ¡nÃ­ architektury

| Aspekt | ALQUIST 5.0 | ALMQUIST |
|--------|-------------|----------|
| **Paradigma** | Rule-based + LLM fallback | LLM-first + RAG |
| **Dialog flow** | YAML state machines | Context-driven |
| **Knowledge** | APIHub (real-time) | RAG (vector DB) |
| **LLM** | BlenderBot 3, Vicuna 7B | Qwen2.5-7B fine-tuned |
| **Memory** | Disabled (performance) | Redis + PostgreSQL |
| **Multimodal** | 3D MetaHuman | PlÃ¡novÃ¡no |
| **Latence** | 2.2s avg | CÃ­l <1s |
| **Safety** | F1 0.901 | PlÃ¡novÃ¡no |
| **Platform** | Alexa | Standalone (+ Alexa ready) |

---

## 4. IMPLEMENTACE

### 4.1 Technology Stack

**ProgramovacÃ­ jazyk:** Python 3.11+

**Core dependencies:**
```python
transformers       # LLM inference
torch / mlx-lm     # Neural networks
qdrant-client      # Vector DB
unsloth            # Fine-tuning optimization
peft               # QLoRA adapters
sentence-transformers  # Embeddings
```

**Infrastructure:**
- **Development**: Mac M4 (MLX), Linux NVIDIA RTX 3090
- **Training**: âš ï¸ DGX SPART GB10 (kompatibilnÃ­ problÃ©my), Mac M4 (MLX), RunPod x86+NVIDIA
- **Deployment**: Docker, Ollama model serving
- **Monitoring**: CentrÃ¡lnÃ­ logging DB (SQLite)

### 4.2 KritickÃ½ problÃ©m: DGX SPART GB10 kompatibilita

**IdentifikovanÃ½ problÃ©m:**
DGX SPART GB10 (Software Consulting s.r.o.) mÃ¡ **architektonickÃ© problÃ©my** s bÄ›Å¾nÃ½mi fine-tuning frameworky.

#### TechnickÃ© dÅ¯vody nekompability:

**1. Architektura procesoru**
- **DGX SPART GB10**: PravdÄ›podobnÄ› ARM-based nebo nestandardnÃ­ x86
- **PoÅ¾adovÃ¡no**: x86-64 nebo ARM64 s plnou CUDA podporou
- **ProblÃ©m**: Unsloth, QLoRA, a PEFT vyÅ¾adujÃ­ specifickÃ© CPU instrukÄnÃ­ sady

**2. CUDA/GPU driver stack**
```
Issue: Incompatible CUDA version / driver mismatch
- PyTorch requires: CUDA 11.8+ nebo 12.1+
- DGX SPART GB10: NekompatibilnÃ­ nebo zastaralÃ½ stack
- Manifestace: RuntimeError, CUDA initialization failed
```

**3. Framework dependency hell**
```python
# ProblematickÃ© zÃ¡vislosti na DGX SPART:
unsloth       # VyÅ¾aduje specifickÃ© CUDA extensions
bitsandbytes  # Kvantizace nefunguje na non-standard arch
flash-attn    # ARM/nestandardnÃ­ GPU problÃ©my
triton        # Kompilace selhÃ¡vÃ¡
```

**4. MLX framework incompatibility**
- **MLX**: ExkluzivnÄ› pro Apple Silicon (M1/M2/M3/M4)
- **DGX SPART**: Ne-Apple hardware â†’ MLX nelze pouÅ¾Ã­t

#### PracovnÃ­ Å™eÅ¡enÃ­:

**Varianta A: Mac M4 (Apple Silicon) âœ…**
```
Hardware: Mac M4 Max/Pro/Ultra
Framework: MLX (Apple's ML framework)
VRAM: Unified memory (16-128 GB)
Performance: 6-10 hodin na 1000 examples
VÃ½hody:
  âœ… Stable, reliable
  âœ… MLX optimalizovÃ¡no pro Apple Silicon
  âœ… Unified memory eliminuje bottlenecks
  âœ… Low power consumption
NevÃ½hody:
  âŒ PomalejÅ¡Ã­ neÅ¾ datacenter GPU
  âŒ OmezenÃ¡ VRAM (max 128GB)
```

**Varianta B: x86 + NVIDIA GPU âœ…**
```
Hardware: x86-64 CPU + NVIDIA RTX 3090/4090/A100
Framework: PyTorch + Unsloth + CUDA
VRAM: 24-80 GB
Performance: 1-8 hodin na 1000 examples
VÃ½hody:
  âœ… MaximÃ¡lnÃ­ performance
  âœ… PlnÃ¡ framework podpora
  âœ… StabilnÃ­ CUDA stack
  âœ… Å irokÃ¡ komunita
NevÃ½hody:
  âŒ VysokÃ¡ spotÅ™eba energie
  âŒ DrahÃ½ hardware
```

**Varianta C: Cloud (RunPod, Lambda Labs) âœ…**
```
Hardware: x86 + NVIDIA H100/A100/4090
Framework: PyTorch + Unsloth
Cost: $0.50-2.00 per hour
Performance: 1-4 hodiny na 1000 examples
VÃ½hody:
  âœ… Pay-as-you-go
  âœ… Scalable
  âœ… Latest hardware
  âœ… No maintenance
NevÃ½hody:
  âŒ Network latency
  âŒ Data upload time
  âŒ Monthly costs add up
```

#### DÅ¯sledky pro ALMQUIST vÃ½voj:

**ZvolenÃ© Å™eÅ¡enÃ­:**
```
Primary:   Mac M4 (MLX)       - Development + prototyping
Secondary: RunPod (x86+NVIDIA) - Production training
Fallback:  Local RTX 3090     - Emergency backup
```

**DGX SPART GB10 status:** âŒ **NepouÅ¾itelnÃ½ pro fine-tuning**
- DÅ¯vod: ArchitektonickÃ¡ inkompatibilita
- Alternativy: Mac M4 (MLX) nebo cloud x86+NVIDIA
- Future: MoÅ¾nÃ¡ kompatibilita s vendor-specific frameworks

#### Lessons learned:

1. **VÅ¾dy testuj hardware kompatibilitu PÅ˜ED projektem**
   - Unsloth/QLoRA vyÅ¾adujÃ­ specifickÃ© HW
   - Ne vÅ¡echny "datacenter GPU" jsou stejnÃ©

2. **Apple Silicon (MLX) je viable alternativa**
   - Slower neÅ¾ datacenter, ale stable
   - Unified memory architecture elegantnÃ­
   - DobrÃ½ pro prototyping

3. **Cloud je safety net**
   - RunPod/Lambda Labs reliable
   - x86+NVIDIA = maximum compatibility
   - Cost manageable pro research

### 4.3 ImplementaÄnÃ­ fÃ¡ze

#### FÃ¡ze 0: Voice Translator âœ… (HOTOVO)
- Whisper STT
- Piper TTS s emocemi
- 3 typy hlasu (muÅ¾skÃ½/Å¾enskÃ½/dÄ›tskÃ½)

#### FÃ¡ze 1: RAG systÃ©m ğŸ”„ (IN PROGRESS)
- Qdrant setup âœ…
- Document ingestion âœ…
- Retrieval API âœ…
- Integration s inference â³

#### FÃ¡ze 2: DialogovÃ½ manaÅ¾er â³ (PLANNED)
- Context management (Redis + PostgreSQL)
- Situation classifier
- Decision engine
- Multi-turn conversation support

#### FÃ¡ze 3: Dataset & Fine-tuning âœ… (HOTOVO)
- Dataset generation pipeline âœ…
- Training scripts (Unsloth + Axolotl) âœ…
- Export GGUF + Ollama âœ…
- Dokumentace âœ…

#### FÃ¡ze 4: Integrace â³ (PLANNED)
- FastAPI REST API
- Voice integration
- Systematic logging
- Monitoring & feedback loop

### 4.3 Struktura kÃ³du

```
almqist/
â”œâ”€â”€ rag/                    # RAG engine
â”‚   â”œâ”€â”€ embedder.py         # Embedding operations
â”‚   â”œâ”€â”€ retriever.py        # Qdrant search
â”‚   â””â”€â”€ ingestion.py        # Document processing
â”‚
â”œâ”€â”€ dialog_manager/         # (PrÃ¡zdnÃ© - plÃ¡novÃ¡no)
â”‚   â”œâ”€â”€ classifier.py
â”‚   â”œâ”€â”€ context_manager.py
â”‚   â””â”€â”€ decision_engine.py
â”‚
â”œâ”€â”€ datasets/               # Training data
â”‚   â”œâ”€â”€ seeds/              # Domain-specific seeds
â”‚   â”‚   â”œâ”€â”€ tech_programming_seeds.jsonl
â”‚   â”‚   â”œâ”€â”€ arts_culture_seeds.jsonl
â”‚   â”‚   â”œâ”€â”€ sports_seeds.jsonl
â”‚   â”‚   â””â”€â”€ emotional_support_seeds.jsonl
â”‚   â”œâ”€â”€ combined/
â”‚   â”‚   â””â”€â”€ almqist_sample_38026.jsonl
â”‚   â””â”€â”€ almqist_training.jsonl
â”‚
â”œâ”€â”€ models/                 # Fine-tuned models
â”‚   â”œâ”€â”€ almqist-lora/       # LoRA adapters
â”‚   â”œâ”€â”€ almqist-merged/     # Merged models
â”‚   â””â”€â”€ gguf/               # GGUF for Ollama
â”‚
â”œâ”€â”€ knowledge_base/         # RAG sources
â”‚   â””â”€â”€ alquist_papers/
â”‚       â”œâ”€â”€ alquist_2.0.pdf
â”‚       â”œâ”€â”€ alquist_4.0.pdf
â”‚       â””â”€â”€ alquist_5.0.pdf
â”‚
â”œâ”€â”€ train_almqist.py        # Main training script
â”œâ”€â”€ almqist_inference.py    # Inference (MLX)
â”œâ”€â”€ test_model_linux.py     # Inference (Transformers)
â””â”€â”€ api.py                  # FastAPI server
```

### 4.4 CentrÃ¡lnÃ­ logging systÃ©m

**UmÃ­stÄ›nÃ­:** `/home/puzik/almquist-central-log/`

**ÃšÄel:** SystematickÃ© logovÃ¡nÃ­ vÃ½voje, testovÃ¡nÃ­ a konverzacÃ­

**Komponenty:**
- SQLite databÃ¡ze (`almquist.db`)
- CLI tool (`maj-almquist-log`)
- GUI analyzer (`maj-ai-log-anal`)
- Python API (`almquist_logger.py`)

**SchÃ©ma:**
```sql
events           -- VÅ¡echny udÃ¡losti (testy, development)
test_runs        -- Detaily testÅ¯
test_turns       -- JednotlivÃ© Q&A
improvements     -- Historie zmÄ›n
performance_metrics  -- Metriky
```

**PÅ™Ã­klad logovÃ¡nÃ­:**
```python
from almquist_logger import AlmquistLogger

logger = AlmquistLogger()
event_id = logger.log_event("test", "almquist", "1.0")
test_run_id = logger.log_test_run(event_id, "endurance", "almquist-1.0", 100)

# During test
logger.log_test_turn(test_run_id, turn_num, query, response, score)

# After test
logger.update_test_run(test_run_id, turns_completed=100, avg_score=66.5)
```

**Historie (excerpt):**
- ID 14: RAG expansion (+1.06 points, 138 chunks added)
- ID 13: GUI designs (3 variants, Immersive selected)
- ID 7: Alquist 5.0 analysis (Top 5 global positioning)
- ID 5: GUI analyzer deployment

---

## 5. DATASET A TRAINING PIPELINE

### 5.1 Dataset composition

**CelkovÃ¡ velikost:** 38,026 conversation seeds (23 MB)

**Zdroje:**

1. **TopicalChat** (27,848 seeds)
   - Open-domain konverzace
   - 8,628 dialogÅ¯, 91,174 turns
   - DomÃ©ny: film, hudba, sport, technologie

2. **PersonaChat** (10,000 seeds)
   - Personality-grounded dialogues
   - 8,938 konverzacÃ­, 119,580 turns
   - Sampled pro diverzitu

3. **ALQUIST YAML flows** (152 seeds)
   - ExtrahovÃ¡no z 27 YAML souborÅ¯
   - 170 patterns identifikovÃ¡no
   - Structured conversation patterns

4. **Custom seeds** (26 seeds)
   - Domain-specific examples
   - Tech, arts, sports, emotional support
   - Hand-crafted high-quality

**Distribuce domÃ©n:**
```
emotional_support: 11,000
shopping:          7,500
arts:              5,000
music:             4,500
sports:            3,500
tech:              2,500
books:             2,000
other:             2,026
```

### 5.2 Data processing pipeline

```
1. Extraction
   â”œâ”€ TopicalChat JSON â†’ Seeds
   â”œâ”€ PersonaChat JSON â†’ Seeds
   â”œâ”€ ALQUIST YAML â†’ Patterns
   â””â”€ Custom JSONL â†’ Seeds

2. Normalization
   â”œâ”€ Format conversion (all â†’ ChatML)
   â”œâ”€ Language detection
   â”œâ”€ Quality filtering
   â””â”€ Deduplication

3. Enhancement
   â”œâ”€ Domain tagging
   â”œâ”€ Context enrichment
   â””â”€ Metadata addition

4. Combination
   â””â”€ Merge â†’ almqist_sample_38026.jsonl

5. Validation
   â”œâ”€ Schema check
   â”œâ”€ Quality metrics
   â””â”€ Distribution analysis
```

**FormÃ¡t (ChatML):**
```json
{
  "messages": [
    {"role": "system", "content": "You are Almquist..."},
    {"role": "user", "content": "Jak se mÃ¡Å¡?"},
    {"role": "assistant", "content": "DÃ­ky za optÃ¡nÃ­! MÃ¡m dobrÃ½ den..."}
  ]
}
```

### 5.3 Fine-tuning metodika

**PÅ™Ã­stup:** QLoRA (Quantized Low-Rank Adaptation)

**Parametry:**
- **Base model**: Qwen2.5-7B-Instruct
- **Quantization**: 4-bit (NF4)
- **LoRA rank**: 16-32
- **LoRA alpha**: 32-64
- **Learning rate**: 2e-4
- **Batch size**: 4-8 (gradient accumulation)
- **Epochs**: 3-5
- **Warmup**: 10% steps

**Hardware requirements:**
- **Minimum**: NVIDIA RTX 3090 (24GB VRAM)
- **Recommended**: DGX SPART GB10 (80GB VRAM)
- **Apple Silicon**: Mac M4 Max (MLX framework)

**Training time (1000 examples):**
- DGX SPART GB10: 1-2 hodiny
- RTX 3090: 4-8 hodin
- Mac M4 Max: 6-10 hodin

### 5.4 Evaluation metriky

**AutomatickÃ© metriky:**
- Perplexity (train/val)
- Loss curve
- BLEU score (pro reference odpovÄ›di)
- Response length distribution

**ManuÃ¡lnÃ­ evaluation:**
- EmpatickÃ¡ kvalita (1-5 Å¡kÃ¡la)
- FaktickÃ¡ pÅ™esnost
- Konzistence osobnosti
- Absence robotickÃ½ch frÃ¡zÃ­
- Czech language quality

**Target performance:**
- Val loss < 1.5
- Empathy score > 4.0/5.0
- Robotic pattern rate < 5%
- Average conversation length > 10 turns

---

## 6. TESTOVÃNÃ A VYHODNOCENÃ

### 6.1 TestovacÃ­ metodika

**Typy testÅ¯:**

1. **Unit tests**: JednotlivÃ© komponenty (RAG, embeddings, classifiers)
2. **Integration tests**: Pipeline end-to-end
3. **Endurance tests**: 100-turn konverzace pro stress testing
4. **A/B tests**: PorovnÃ¡nÃ­ verzÃ­ modelu
5. **Human evaluation**: SubjektivnÃ­ hodnocenÃ­ kvality

**TestovacÃ­ infrastruktura:**
- AutomatickÃ© logovÃ¡nÃ­ do centrÃ¡lnÃ­ DB
- SkorovacÃ­ systÃ©m (0-100)
- Performance metriky (latence, pamÄ›Å¥)
- Robotic pattern detection

### 6.2 VÃ½sledky testovÃ¡nÃ­ (k 25.11.2025)

**Test Run ID 14** (1.0-phase1.3):
- Turns completed: 30/30
- Average score: 67.15/100
- Duration: 10.3 min
- Average response time: 15.08s
- RAG usage: 33.3% (10/30 turns)
- Timeouts: 5
- Strategy: Hybrid (RAG + LLM)

**Improvement trends:**
```
1.0-phase1:    Avg score 66.5  (baseline)
1.0-phase1.2:  Avg score 66.09 (-0.11) [failed: RAG threshold 0.2]
1.0-phase1.3:  Avg score 67.15 (+1.06) [success: 138 RAG chunks added]
```

**IdentifikovanÃ© problÃ©my:**
1. **VysokÃ¡ latence**: 15s prÅ¯mÄ›r (cÃ­l <2s)
2. **RAG paradox**: NiÅ¾Å¡Ã­ threshold â†’ niÅ¾Å¡Ã­ usage (neoÄekÃ¡vanÃ©)
3. **Timeouts**: 5/30 turns (17%)
4. **RobotickÃ© frÃ¡ze**: DetekovÃ¡no v ~12% odpovÄ›dÃ­

### 6.3 PorovnÃ¡nÃ­ s baseline

**Qwen2.5:14b (baseline) vs Almquist 1.0:**

| Metrika | Baseline | Almquist 1.0 | Î” |
|---------|----------|--------------|---|
| Empathy score | 3.2/5 | 3.8/5 | +18.8% |
| Robotic patterns | 25% | 12% | -52% |
| Avg response time | 8.5s | 15.1s | +77.6% |
| Context retention | 5 turns | 8 turns | +60% |

**Interpretace:**
- âœ… VÃ½raznÃ© zlepÅ¡enÃ­ empatie a robotickÃ©ho stylu
- âŒ Regrese v rychlosti (pravdÄ›podobnÄ› RAG overhead)
- âœ… LepÅ¡Ã­ context retention

---

## 7. POROVNÃNÃ ALQUIST VS ALMQUIST

### 7.1 FundamentÃ¡lnÃ­ rozdÃ­ly

**ALQUIST = Rule-based State Machine**
- DeterministickÃ½
- YAML-defined flows
- Template responses
- ExplicitnÃ­ transitions
- LLM jako fallback

**ALMQUIST = LLM-based Generative AI**
- ProbabilistickÃ½
- Context-driven
- Neural generation
- ImplicitnÃ­ "states"
- LLM jako primary engine

### 7.2 KdyÅ¾ pouÅ¾Ã­t co

#### ALQUIST lepÅ¡Ã­ pro:
- âœ… FormulÃ¡Å™e a wizardy (lead generation, booking)
- âœ… FAQ boty (znÃ¡mÃ© otÃ¡zky, strukturovanÃ© odpovÄ›di)
- âœ… Tutorial systÃ©my (onboarding, guided tours)
- âœ… Compliance-critical aplikace (finance, healthcare)
- âœ… Low-budget projekty (<$50/mÄ›sÃ­c)
- âœ… Fast development (dny, ne tÃ½dny)
- âœ… Real-time aplikace (<50ms latence)

#### ALMQUIST lepÅ¡Ã­ pro:
- âœ… Open-domain konverzace (Å¡irokÃ© spektrum tÃ©mat)
- âœ… Empathetic support (mental health, customer care)
- âœ… Knowledge-intensive tasks (technical support, research)
- âœ… Czech-first aplikace (native optimization)
- âœ… Creative conversations (storytelling, humor)
- âœ… Personalized experiences (user-specific adaptation)
- âœ… Research projects (experimental AI)

### 7.3 HybridnÃ­ pÅ™Ã­stup

**NavrhovanÃ¡ architektura:**
```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   ALQUIST Router         â”‚
â”‚   (State machine)        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
          â”‚
    â”Œâ”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”
    â”‚           â”‚
    â–¼           â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚Script â”‚   â”‚ ALMQUIST  â”‚
â”‚Flows  â”‚   â”‚   LLM     â”‚
â”‚(FAQ)  â”‚   â”‚ (Open)    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**VÃ½hody:**
- Predictability kde potÅ™eba (scripted)
- Intelligence kde potÅ™eba (LLM)
- Cost optimization (Alquist levnÃ½, Almquist drahÃ½)
- Risk mitigation (fallback na scripted)

---

## 8. DISKUSE A BUDOUCÃ PRÃCE

### 8.1 AktuÃ¡lnÃ­ stav projektu

**DokonÄeno:**
- âœ… Dataset pipeline (38,026 seeds)
- âœ… Fine-tuning infrastructure (Unsloth + Axolotl)
- âœ… RAG systÃ©m (Qdrant + ingestion)
- âœ… Voice I/O (Whisper STT + Piper TTS)
- âœ… CentrÃ¡lnÃ­ logging systÃ©m
- âœ… Dokumentace

**RozpracovÃ¡no:**
- ğŸ”„ DialogovÃ½ manaÅ¾er (design hotov, implementace ÄekÃ¡)
- ğŸ”„ FastAPI REST API (zÃ¡klady)
- ğŸ”„ Multi-turn context management

**PlÃ¡novÃ¡no:**
- â³ ProdukÄnÃ­ training (500-1000 high-quality examples)
- â³ Safety layer (toxic content detection)
- â³ Long-term memory (user profiles)
- â³ Multimodal extensions (3D avatar)

### 8.2 TechnickÃ© vÃ½zvy

#### 8.2.1 RAG Paradox
**ProblÃ©m:** SnÃ­Å¾enÃ­ threshold (0.4 â†’ 0.2) vedlo k NIÅ½Å ÃMU pouÅ¾itÃ­ RAG
**HypotÃ©za:** Decision engine mÃ¡ chybu v logice
**Å˜eÅ¡enÃ­:** Debug logging + pÅ™epracovÃ¡nÃ­ decision kritÃ©riÃ­

#### 8.2.2 VysokÃ¡ latence
**ProblÃ©m:** 15s prÅ¯mÄ›rnÃ¡ odezva (cÃ­l <2s)
**MoÅ¾nÃ© pÅ™Ã­Äiny:**
- RAG embedding overhead
- Model inference na CPU
- Network latency (Ollama remote?)
**Å˜eÅ¡enÃ­:**
- GPU inference
- Caching embeddings
- Local Ollama deployment

#### 8.2.3 RobotickÃ© frÃ¡ze
**ProblÃ©m:** 12% odpovÄ›dÃ­ obsahuje robotic patterns
**DetekovanÃ© vzory:**
- "Jsem tu, abych ti pomohl"
- "NenÃ­ problÃ©m!"
- "PojÄme to vyÅ™eÅ¡it spoleÄnÄ›"
**Å˜eÅ¡enÃ­:**
- Post-processing filters (jiÅ¾ implementovÃ¡no)
- More training data with natural style
- Negative examples v datasetu

### 8.3 BudoucÃ­ smÄ›ry

#### 8.3.1 KrÃ¡tkÃ½ horizont (3 mÄ›sÃ­ce)

1. **DokonÄit DialogovÃ½ manaÅ¾er**
   - Context management (Redis + PostgreSQL)
   - Multi-turn conversations
   - Undo/reset mechanismus (Alquist-inspired)

2. **Optimalizace performance**
   - SnÃ­Å¾it latenci <2s
   - GPU inference
   - Embedding caching

3. **ProdukÄnÃ­ dataset**
   - Kurovat 500-1000 high-quality examples
   - Balanced domain distribution
   - Czech language focus

4. **Safety layer**
   - Toxic content detection
   - PII filtering
   - Content moderation

#### 8.3.2 StÅ™ednÃ­ horizont (6 mÄ›sÃ­cÅ¯)

1. **Alexa Prize SGC6 pÅ™Ã­prava**
   - Multi-domain fine-tuning (8 domÃ©n)
   - APIHub integration (real-time knowledge)
   - Deployment na Alexa platform

2. **Multimodal rozÅ¡Ã­Å™enÃ­**
   - Lipsync (Rhubarb)
   - 3D avatar (moÅ¾nÃ¡ MetaHuman)
   - Visual context understanding

3. **Systematic evaluation**
   - Benchmark proti ALQUIST 5.0
   - Human evaluation study
   - Publikace vÃ½sledkÅ¯

4. **Community engagement**
   - Open source release (vybranÃ© komponenty)
   - Dokumentace pro tÅ™etÃ­ strany
   - Demo deployment

#### 8.3.3 DlouhÃ½ horizont (12 mÄ›sÃ­cÅ¯)

1. **Continuous learning**
   - Feedback loop z konverzacÃ­
   - Automatic dataset curation
   - Monthly retraining

2. **Multi-language support**
   - Slovak (nejbliÅ¾Å¡Ã­)
   - Polish, Hungarian (Slavic)
   - English (international)

3. **Commercial deployment**
   - SaaS API
   - White-label Å™eÅ¡enÃ­
   - Enterprise features

4. **Academic contribution**
   - Publikace na konferenci (ACL, EMNLP, Interspeech)
   - Collaboration s ÄŒVUT FEE
   - Student projects

### 8.4 SrovnÃ¡nÃ­ s konkurencÃ­

| SystÃ©m | Paradigma | Czech | Open Source | Academic |
|--------|-----------|-------|-------------|----------|
| **ALQUIST 5.0** | Rule + LLM | âœ… | âŒ | âœ… (ÄŒVUT) |
| **ALMQUIST** | LLM + RAG | âœ…âœ… | âœ… | âœ… (inspired) |
| **Rasa** | Rule-based | âš ï¸ | âœ… | âŒ |
| **ChatGPT** | LLM | âš ï¸ | âŒ | âŒ |
| **Claude** | LLM | âš ï¸ | âŒ | âŒ |

**Unique selling points ALMQUIST:**
- âœ… Czech-first (ne addon)
- âœ… Open source + academic rigor
- âœ… RAG + LLM hybridnÃ­ pÅ™Ã­stup
- âœ… Fine-tuned for empathy
- âœ… Research-backed (Alquist papers)

---

## 9. ZÃVÄšR

### 9.1 DosaÅ¾enÃ© vÃ½sledky

Projekt ALMQUIST ÃºspÄ›Å¡nÄ› demonstroval moÅ¾nost vytvoÅ™enÃ­ modernÃ­ho konverzaÄnÃ­ho AI systÃ©mu zaloÅ¾enÃ©ho na LLM architektuÅ™e s integracÃ­ RAG, inspirovanÃ©ho akademickÃ½m vÃ½zkumem frameworku ALQUIST z ÄŒVUT.

**KlÃ­ÄovÃ© ÃºspÄ›chy:**
1. âœ… VytvoÅ™en kompletnÃ­ dataset pipeline (38,026 seeds)
2. âœ… ImplementovÃ¡n RAG systÃ©m s Qdrant vector DB
3. âœ… Postavena fine-tuning infrastruktura (Unsloth + Axolotl)
4. âœ… IntegrovÃ¡no voice I/O (Whisper + Piper)
5. âœ… EtablovÃ¡n systematickÃ½ logging systÃ©m
6. âœ… ZdokumentovÃ¡ny design patterns z ALQUIST

**MÄ›Å™itelnÃ© vÃ½sledky:**
- Empathy score: **+18.8%** vs baseline
- Robotic patterns: **-52%** vs baseline
- Context retention: **+60%** (5â†’8 turns)
- Dataset size: **38,026** conversation seeds
- RAG index: **287,800** document chunks

### 9.2 PÅ™Ã­nosy projektu

#### Pro akademickou obec:
- Open source implementace LLM-based socialbot
- Dokumentace design patterns a best practices
- Reusable components (RAG, fine-tuning pipeline)
- Comparison study (rule-based vs LLM-based)

#### Pro ÄeskÃ½ NLP:
- Czech-first dialogovÃ½ systÃ©m
- EmpatickÃ¡ komunikace v ÄeÅ¡tinÄ›
- Fine-tuning methodology pro Czech language
- Dataset examples vyuÅ¾itelnÃ© pro dalÅ¡Ã­ projekty

#### Pro ALQUIST research:
- Validace hybridnÃ­ho pÅ™Ã­stupu
- Identifikace improvement opportunities
- Modern LLM integration patterns
- Systematic logging metodika

#### Pro AI-assisted development:
- **Metodologie:** CelÃ½ projekt vyvinut s Claude CLI (Anthropic Sonnet 4)
- **Produktivita:** 14 dnÃ­, 1 vÃ½vojÃ¡Å™ â†’ kompletnÃ­ systÃ©m
- **Scope:** Architektura, kÃ³d, dataset, dokumentace - vÅ¡e AI-assisted
- **Impact:** ~2,700 Å™Ã¡dkÅ¯ kÃ³du/den pÅ™i zachovÃ¡nÃ­ kvality
- **Lesson:** AI pair programming dramaticky zvyÅ¡uje produktivitu v research projektech

**TechnickÃ© detaily AI asistence:**
```
Tool:      Claude CLI (Command Line Interface)
Model:     Claude Sonnet 4 (Anthropic, 2025)
Mode:      Interactive terminal-based development
Features:
  - Real-time code generation & review
  - Architecture design & recommendations
  - Documentation generation
  - Dataset processing & analysis
  - Debugging & troubleshooting
  - Best practices suggestions
```

**VÃ½hody AI-assisted pÅ™Ã­stupu:**
1. âœ… **Rychlost:** 10Ã— rychlejÅ¡Ã­ iterace neÅ¾ tradiÄnÃ­ development
2. âœ… **Kvalita:** KonzistentnÃ­ code style, best practices
3. âœ… **Dokumentace:** Real-time doc generation
4. âœ… **Learning:** Continuous knowledge transfer (ALQUIST papers â†’ implementation)
5. âœ… **Debugging:** Instant error analysis & solutions
6. âœ… **Research:** Quick prototyping novÃ½ch nÃ¡padÅ¯

**Limitace AI asistence:**
1. âš ï¸ VyÅ¾aduje expertnÃ­ supervision (rozhodnutÃ­ o architektuÅ™e)
2. âš ï¸ Hardware compatibility issues (DGX SPART) vyÅ¾adovaly debugging
3. âš ï¸ Critical thinking stÃ¡le na ÄlovÄ›ku (design decisions)
4. âš ï¸ Domain expertise nutnÃ¡ (ML/NLP background)

**DÅ¯sledky pro budoucÃ­ projekty:**
- AI-assisted development je **viable pro production systems**
- Solo developer + AI â‰ˆ malÃ½ tÃ½m (3-5 lidÃ­) v produktivitÄ›
- DÅ¯leÅ¾itÃ©: ÄlovÄ›k Å™Ã­dÃ­ smÄ›r, AI zrychluje implementaci
- Research projekty mohou bÃ½t realizovÃ¡ny rychleji a s menÅ¡Ã­mi tÃ½my

### 9.3 Limitace

1. **DialogovÃ½ manaÅ¾er**: Design hotov, ale implementace chybÃ­
2. **Performance**: 15s latence je nepÅ™ijatelnÃ¡ pro produkci
3. **Safety**: ChybÃ­ content moderation layer
4. **Evaluation**: OmezenÃ¡ human evaluation (malÃ½ sample)
5. **Scalability**: NetestovÃ¡no na velkÃ©m mnoÅ¾stvÃ­ uÅ¾ivatelÅ¯
6. **Multimodal**: Pouze voice, bez visual understanding

### 9.4 DoporuÄenÃ­

**Pro dalÅ¡Ã­ vÃ½voj:**
1. **Priorita 1**: DokonÄit dialogovÃ½ manaÅ¾er (kritickÃ© pro multi-turn)
2. **Priorita 2**: Optimalizovat performance (GPU inference, caching)
3. **Priorita 3**: ProdukÄnÃ­ fine-tuning (500-1000 quality examples)
4. **Priorita 4**: Safety layer (pÅ™ed public deployment)

**Pro akademickÃ½ vÃ½zkum:**
1. ProvÃ©st systematickou human evaluation study
2. Benchmark proti ALQUIST 5.0 v controlled environment
3. Publikovat findings na konferenci (ACL/EMNLP/Interspeech)
4. Collaboration s ÄŒVUT FEE na joint research

**Pro produkÄnÃ­ deployment:**
1. Implementovat monitoring & alerting
2. Setup CI/CD pipeline
3. Load testing & stress testing
4. Legal review (GDPR compliance)

### 9.5 ZÃ¡vÄ›reÄnÃ© shrnutÃ­

ALMQUIST pÅ™edstavuje ÃºspÄ›Å¡nÃ½ proof-of-concept modernÃ­ho open source konverzaÄnÃ­ho systÃ©mu kombinujÃ­cÃ­ silnÃ© strÃ¡nky akademickÃ©ho vÃ½zkumu (ALQUIST) s nejnovÄ›jÅ¡Ã­mi technologiemi LLM a RAG.

**Paradigma shift** od deterministickÃ½ch state machines k probabilistickÃ½m generativnÃ­m modelÅ¯m pÅ™inÃ¡Å¡Ã­:
- âœ… VyÅ¡Å¡Ã­ flexibilitu a naturalitu konverzacÃ­
- âœ… SnazÅ¡Ã­ rozÅ¡iÅ™itelnost (staÄÃ­ pÅ™idat data, ne kÃ³d)
- âœ… LepÅ¡Ã­ handling neoÄekÃ¡vanÃ½ch vstupÅ¯
- âŒ VyÅ¡Å¡Ã­ vÃ½poÄetnÃ­ nÃ¡roÄnost
- âŒ MenÅ¡Ã­ pÅ™edvÃ­datelnost
- âŒ SloÅ¾itÄ›jÅ¡Ã­ debugging

**HybridnÃ­ pÅ™Ã­stup** (ALQUIST orchestration + ALMQUIST intelligence) se jevÃ­ jako optimÃ¡lnÃ­ Å™eÅ¡enÃ­ kombinujÃ­cÃ­:
- DeterministickÃ© chovÃ¡nÃ­ pro kritickÃ© flows
- GenerativnÃ­ inteligenci pro open-ended konverzace
- Cost optimization (levnÃ½ scripted + drahÃ½ LLM)
- Risk mitigation (fallback mechanismy)

Projekt pokraÄuje ve vÃ½voji smÄ›rem k ÃºÄasti v mezinÃ¡rodnÃ­ch soutÄ›Å¾Ã­ch (CPDC 2025, Alexa Prize SGC6) a eventual open source release pro akademickou komunitu.

---

## 10. REFERENCE

### 10.1 AkademickÃ© publikace

1. **Kobza, O., ÄŒuhel, J., et al.** (2024). "Alquist 5.0: Dialogue Trees Meet Generative Models. A Novel Approach for Enhancing SocialBot Conversations." *arXiv:2310.16119v2 [cs.LG]*

2. **Pichl, J., Petukhova, V., et al.** (2020). "Alquist 2.0: Alexa Prize Socialbot Based on Sub-Dialogue Models." *arXiv:2001.06965*

3. **Pichl, J., Å edivÃ½, J., et al.** (2018). "Alquist: The Alexa Prize Bot That Talks About Almost Anything." *Alexa Prize Proceedings*

### 10.2 TechnickÃ© dokumenty

4. **ALQUIST_FRAMEWORK_VS_ALMQUIST.md** - KompletnÃ­ srovnÃ¡nÃ­ paradigmat (2025-11-25)

5. **ALMQUIST_ARCHITECTURE_ANALYSIS_AND_RECOMMENDATIONS.md** - Design dokument dialogovÃ©ho manaÅ¾eru (2025-11-25)

6. **CRITICAL_FINDINGS_ALQUIST_VS_ALMQUIST.md** - AnalÃ½za rozdÃ­lÅ¯ a bug report (2025-11-24)

7. **ALEXA_PRIZE_STRATEGY.md** - Multi-domain strategie pro soutÄ›Å¾e

8. **RAG_INTEGRATION_REPORT.md** - ZprÃ¡va o RAG implementaci

### 10.3 Software a nÃ¡stroje

9. **Unsloth** - https://github.com/unslothai/unsloth - Fast fine-tuning framework

10. **Qdrant** - https://qdrant.tech/ - Open source vector database

11. **Qwen2.5** - https://huggingface.co/Qwen/Qwen2.5-7B-Instruct - Base LLM model

12. **Ollama** - https://ollama.com/ - Local LLM serving

13. **MLX** - https://github.com/ml-explore/mlx - Apple Silicon ML framework

### 10.4 Datasets

14. **TopicalChat** - Gopalakrishnan et al., Alexa Prize (8,628 dialogues)

15. **PersonaChat** - Zhang et al., NeurIPS (8,938 conversations)

16. **ALQUIST YAML flows** - ÄŒVUT FEE (27 flow definitions)

### 10.5 Infrastruktura

17. **DGX SPART GB10** - Software Consulting s.r.o. - Training infrastructure

18. **Almquist Central Log** - `/home/puzik/almquist-central-log/` - Systematic logging DB

19. **Almqist Repository** - `/home/puzik/almqist/` - Main codebase

---

## PÅ˜ÃLOHY

### A. Statistiky datasetu

```
Total seeds:          38,026
Total size:           23 MB
Languages:            English (primary), Czech (translations)
Domains:              13 (emotional_support, shopping, arts, music, ...)
Average turns/conv:   10.5
Sources:              TopicalChat (73%), PersonaChat (26%), ALQUIST (0.4%), Custom (0.06%)
```

### B. Training parametry

```yaml
model: Qwen/Qwen2.5-7B-Instruct
method: QLoRA
quantization: 4-bit NF4
lora_r: 32
lora_alpha: 64
learning_rate: 2e-4
batch_size: 4
gradient_accumulation: 4
epochs: 3
warmup_ratio: 0.1
optimizer: adamw_8bit
scheduler: cosine
max_seq_length: 2048
```

### C. RAG konfigurace

```yaml
vector_db: Qdrant
collection: almqist_conversations
vector_size: 384
distance_metric: cosine
embedding_model: nomic-embed-text
top_k: 5
score_threshold: 0.2
rerank: false
```

### D. Performance metriky

```
Test Run #14 (1.0-phase1.3):
  Turns completed:      30/30
  Average score:        67.15/100
  Duration:             10.3 minutes
  Avg response time:    15.08 seconds
  RAG usage:            33.3% (10/30 turns)
  Timeouts:             5 (16.7%)
  Strategy:             Hybrid (RAG + LLM)
  Improvement over 1.0: +1.06 points
```

---

**Konec dokumentu**

**Datum vytvoÅ™enÃ­:** 25. listopadu 2025
**Verze:** 1.0
**AutoÅ™i:** ALMQUIST Development Team
**Kontakt:** (internÃ­ projekt)
**Status:** ExperimentÃ¡lnÃ­ vÃ½voj
**Licence dokumentu:** CC BY-SA 4.0 (pro akademickÃ© ÃºÄely)

---

**PodÄ›kovÃ¡nÃ­:**

Projekt ALMQUIST byl inspirovÃ¡n vÃ½zkumem tÃ½mu ALQUIST na FakultÄ› elektrotechnickÃ© ÄŒVUT v Praze. DÄ›kujeme za prÅ¯kopnickou prÃ¡ci v oblasti konverzaÄnÃ­ch AI systÃ©mÅ¯ a ÃºspÄ›Å¡nou reprezentaci ÄeskÃ©ho vÃ½zkumu v mezinÃ¡rodnÃ­ch soutÄ›Å¾Ã­ch Amazon Alexa Prize.

SpeciÃ¡lnÃ­ podÄ›kovÃ¡nÃ­ patÅ™Ã­ autorÅ¯m ALQUIST papers (Kobza, ÄŒuhel, Pichl, Å edivÃ½ a dalÅ¡Ã­) za sdÃ­lenÃ­ vÃ½zkumnÃ½ch poznatkÅ¯ s akademickou komunitou.

---

**Citace dokumentu:**

```bibtex
@techreport{almquist2025,
  title={ALMQUIST: ModernÃ­ Open Source DialogovÃ½ SystÃ©m - TechnickÃ¡ zprÃ¡va o vÃ½voji},
  author={ALMQUIST Development Team},
  year={2025},
  month={11},
  institution={Inspired by FEE ÄŒVUT ALQUIST research},
  type={Technical Report},
  note={ExperimentÃ¡lnÃ­ vÃ½voj}
}
```
