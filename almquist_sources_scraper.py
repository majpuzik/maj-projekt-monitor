#!/usr/bin/env python3
"""
ALMQUIST RAG - Sources Scraper
AutomatickÃ© scrapovÃ¡nÃ­ oficiÃ¡lnÃ­ch ÄeskÃ½ch zdrojÅ¯ pro aktualizaci RAG databÃ¡ze
"""

import requests
from bs4 import BeautifulSoup
import json
import re
from datetime import datetime
from pathlib import Path
import time

class AlmquistSourcesScraper:
    """Scraper pro oficiÃ¡lnÃ­ ÄeskÃ© zdroje"""

    def __init__(self):
        self.session = requests.Session()
        self.session.headers.update({
            'User-Agent': 'Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36'
        })
        self.updates = []
        self.log = []

    def log_info(self, message):
        """Log zprÃ¡vy"""
        timestamp = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
        log_msg = f"[{timestamp}] {message}"
        self.log.append(log_msg)
        print(log_msg)

    def scrape_cssz_rates(self):
        """Scrape ÄŒSSZ - minimÃ¡lnÃ­ zÃ¡lohy na sociÃ¡lnÃ­ pojiÅ¡tÄ›nÃ­"""
        self.log_info("ğŸ“Š Scraping ÄŒSSZ - minimÃ¡lnÃ­ zÃ¡lohy...")

        try:
            url = "https://www.cssz.cz/minimalni-zalohy-na-pojistne"
            response = self.session.get(url, timeout=10)

            if response.status_code == 200:
                soup = BeautifulSoup(response.text, 'html.parser')

                # Hledat aktuÃ¡lnÃ­ sazby (2025)
                text = soup.get_text()

                # Extrahovat minimÃ¡lnÃ­ zÃ¡lohu
                pattern = r'(?:minimÃ¡lnÃ­ zÃ¡loha|minimÃ¡lnÃ­ mÄ›sÃ­ÄnÃ­ zÃ¡loha).*?(\d{1,2}[\s]?\d{3}).*?KÄ'
                matches = re.findall(pattern, text, re.IGNORECASE)

                if matches:
                    min_payment = int(matches[0].replace(' ', ''))
                    current_year = datetime.now().year

                    update = {
                        'source': 'cssz.cz',
                        'type': 'social_insurance',
                        'data': {
                            'min_monthly_payment': min_payment,
                            'year': current_year,
                            'deadline': '20. den nÃ¡sledujÃ­cÃ­ho mÄ›sÃ­ce'
                        },
                        'scraped_at': datetime.now().isoformat(),
                        'url': url
                    }

                    self.updates.append(update)
                    self.log_info(f"   âœ“ MinimÃ¡lnÃ­ zÃ¡loha OSVÄŒ: {min_payment} KÄ ({current_year})")
                    return update

        except Exception as e:
            self.log_info(f"   âœ— Chyba pÅ™i scrapingu ÄŒSSZ: {e}")

        return None

    def scrape_health_insurance_rates(self):
        """Scrape VZP - minimÃ¡lnÃ­ pojistnÃ©"""
        self.log_info("ğŸ¥ Scraping VZP - zdravotnÃ­ pojiÅ¡tÄ›nÃ­...")

        try:
            url = "https://www.vzp.cz/platci/informace/platby-pojistneho"
            response = self.session.get(url, timeout=10)

            if response.status_code == 200:
                soup = BeautifulSoup(response.text, 'html.parser')
                text = soup.get_text()

                # Hledat minimÃ¡lnÃ­ pojistnÃ©
                pattern = r'(?:minimÃ¡lnÃ­.*?pojistnÃ©|OSVÄŒ.*?minimÃ¡lnÃ­).*?(\d{1,2}[\s]?\d{3}).*?KÄ'
                matches = re.findall(pattern, text, re.IGNORECASE)

                if matches:
                    min_payment = int(matches[0].replace(' ', ''))
                    current_year = datetime.now().year

                    update = {
                        'source': 'vzp.cz',
                        'type': 'health_insurance',
                        'data': {
                            'min_monthly_payment': min_payment,
                            'year': current_year,
                            'percentage': 13.5,
                            'deadline': '8. den nÃ¡sledujÃ­cÃ­ho mÄ›sÃ­ce'
                        },
                        'scraped_at': datetime.now().isoformat(),
                        'url': url
                    }

                    self.updates.append(update)
                    self.log_info(f"   âœ“ MinimÃ¡lnÃ­ pojistnÃ© OSVÄŒ: {min_payment} KÄ ({current_year})")
                    return update

        except Exception as e:
            self.log_info(f"   âœ— Chyba pÅ™i scrapingu VZP: {e}")

        return None

    def scrape_financial_administration(self):
        """Scrape FinanÄnÃ­ sprÃ¡va - DPH sazby, termÃ­ny"""
        self.log_info("ğŸ’° Scraping FinanÄnÃ­ sprÃ¡va - DPH...")

        try:
            url = "https://www.financnisprava.cz/cs/dane/danove-tiskopisy-a-zakladni-informace/dph"
            response = self.session.get(url, timeout=10)

            if response.status_code == 200:
                soup = BeautifulSoup(response.text, 'html.parser')
                text = soup.get_text()

                # Hledat zÃ¡kladnÃ­ sazbu DPH
                pattern = r'(?:zÃ¡kladnÃ­ sazba|standardnÃ­ sazba).*?(\d+)\s*%'
                matches = re.findall(pattern, text, re.IGNORECASE)

                if matches:
                    vat_rate = int(matches[0])

                    update = {
                        'source': 'financnisprava.cz',
                        'type': 'vat',
                        'data': {
                            'standard_rate': vat_rate,
                            'reduced_rates': [12, 0],  # SnÃ­Å¾enÃ© sazby
                            'deadline_monthly': '25. den nÃ¡sledujÃ­cÃ­ho mÄ›sÃ­ce',
                            'tax_return_deadline': '1. duben nÃ¡sledujÃ­cÃ­ho roku'
                        },
                        'scraped_at': datetime.now().isoformat(),
                        'url': url
                    }

                    self.updates.append(update)
                    self.log_info(f"   âœ“ ZÃ¡kladnÃ­ sazba DPH: {vat_rate}%")
                    return update

        except Exception as e:
            self.log_info(f"   âœ— Chyba pÅ™i scrapingu FinanÄnÃ­ sprÃ¡vy: {e}")

        return None

    def scrape_czso_statistics(self):
        """Scrape ÄŒSÃš - statistiky pÅ™Ã­jmÅ¯"""
        self.log_info("ğŸ“ˆ Scraping ÄŒSÃš - statistiky pÅ™Ã­jmÅ¯...")

        try:
            # ÄŒSÃš mÃ¡ komplexnÃ­ API, pouÅ¾ijeme zjednoduÅ¡enÃ½ pÅ™Ã­stup
            url = "https://www.czso.cz/csu/czso/prace_a_mzdy_prace"
            response = self.session.get(url, timeout=10)

            if response.status_code == 200:
                update = {
                    'source': 'czso.cz',
                    'type': 'statistics',
                    'data': {
                        'note': 'Statistiky se aktualizujÃ­ ÄtvrtletnÄ›',
                        'last_check': datetime.now().isoformat()
                    },
                    'scraped_at': datetime.now().isoformat(),
                    'url': url
                }

                self.updates.append(update)
                self.log_info(f"   âœ“ ÄŒSÃš statistiky zkontrolovÃ¡ny")
                return update

        except Exception as e:
            self.log_info(f"   âœ— Chyba pÅ™i scrapingu ÄŒSÃš: {e}")

        return None

    def scrape_cak_fees(self):
        """Scrape ÄŒAK - pÅ™Ã­spÄ›vky advokÃ¡tÅ¯"""
        self.log_info("âš–ï¸  Scraping ÄŒAK - pÅ™Ã­spÄ›vky advokÃ¡tÅ¯...")

        try:
            url = "https://www.cak.cz/scripts/detail.php?id=16690"
            response = self.session.get(url, timeout=10)

            if response.status_code == 200:
                soup = BeautifulSoup(response.text, 'html.parser')
                text = soup.get_text()

                # Hledat mÄ›sÃ­ÄnÃ­ pÅ™Ã­spÄ›vek
                pattern = r'(?:mÄ›sÃ­ÄnÃ­ pÅ™Ã­spÄ›vek).*?(\d+).*?KÄ'
                matches = re.findall(pattern, text, re.IGNORECASE)

                if matches:
                    monthly_fee = int(matches[0])

                    update = {
                        'source': 'cak.cz',
                        'type': 'chamber_fees',
                        'profession': 'advokat',
                        'data': {
                            'monthly_fee': monthly_fee,
                            'insurance_requirement': 25000,
                            'year': datetime.now().year
                        },
                        'scraped_at': datetime.now().isoformat(),
                        'url': url
                    }

                    self.updates.append(update)
                    self.log_info(f"   âœ“ MÄ›sÃ­ÄnÃ­ pÅ™Ã­spÄ›vek ÄŒAK: {monthly_fee} KÄ")
                    return update

        except Exception as e:
            self.log_info(f"   âœ— Chyba pÅ™i scrapingu ÄŒAK: {e}")

        return None

    def scrape_all(self):
        """Scrape vÅ¡echny zdroje"""
        self.log_info("\n" + "="*70)
        self.log_info("ALMQUIST RAG - AutomatickÃ¡ aktualizace ze zdrojÅ¯")
        self.log_info("="*70 + "\n")

        sources = [
            self.scrape_cssz_rates,
            self.scrape_health_insurance_rates,
            self.scrape_financial_administration,
            self.scrape_czso_statistics,
            self.scrape_cak_fees
        ]

        for scraper in sources:
            try:
                scraper()
                time.sleep(2)  # BÃ½t sluÅ¡nÃ½ k serverÅ¯m
            except Exception as e:
                self.log_info(f"âœ— Chyba pÅ™i scrapingu: {e}")

        self.log_info(f"\nâœ… Scraping dokonÄen. Nalezeno {len(self.updates)} aktualizacÃ­.")
        return self.updates

    def save_updates(self, filepath="/home/puzik/almquist_rag_updates.json"):
        """UloÅ¾it aktualizace"""
        output = {
            'updates': self.updates,
            'log': self.log,
            'scraped_at': datetime.now().isoformat(),
            'total_updates': len(self.updates)
        }

        with open(filepath, 'w', encoding='utf-8') as f:
            json.dump(output, f, indent=2, ensure_ascii=False)

        self.log_info(f"\nğŸ’¾ Aktualizace uloÅ¾eny do: {filepath}")
        return filepath


def main():
    scraper = AlmquistSourcesScraper()
    updates = scraper.scrape_all()
    scraper.save_updates()

    return updates


if __name__ == "__main__":
    main()
