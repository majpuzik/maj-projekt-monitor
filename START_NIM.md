# üöÄ NVIDIA NIM na DGX Spark - Kompletn√≠ n√°vod

**Datum vytvo≈ôen√≠:** 14.11.2025
**Syst√©m:** DGX Spark GB10 (ARM64), CUDA 13.0

---

## üìñ Co je NVIDIA NIM?

**NVIDIA NIM (NVIDIA Inference Microservices)** je kontejnerizovan√Ω software pro rychl√© a spolehliv√© poskytov√°n√≠ AI model≈Ø a inference na NVIDIA GPU.

### V√Ωhody NIM:
- ‚úÖ **Production-ready** - optimalizov√°no pro produkci
- ‚úÖ **GPU-accelerated** - pln√© vyu≈æit√≠ GB10
- ‚úÖ **OpenAI-compatible API** - snadn√° integrace
- ‚úÖ **P≈ôedkonfigurovan√©** - ≈æ√°dn√© slo≈æit√© nastaven√≠
- ‚úÖ **ARM64 podpora** - speci√°ln√≠ containery pro DGX Spark

---

## üéØ Dostupn√© modely pro DGX Spark

### 1. Llama 3.1 8B Instruct
- **Container:** `nvcr.io/nim/meta/llama-3.1-8b-instruct:1.2.4`
- **Velikost:** ~10GB
- **Pamƒõ≈•:** ~16GB GPU RAM
- **Vhodn√© pro:** Obecn√© text generation, chat, Q&A

### 2. Qwen3-32B (Spark-optimized)
- **Container:** `nvcr.io/nim/qwen/qwen3-32b-dgx-spark:latest`
- **Velikost:** ~20GB
- **Pamƒõ≈•:** ~40GB GPU RAM
- **Vhodn√© pro:** Advanced reasoning, multimodal tasks
- **Pozn√°mka:** Speci√°ln√≠ verze optimalizovan√° pro DGX Spark!

---

## üîë Krok 1: Z√≠sk√°n√≠ NGC API Key

### A) P≈ôejdƒõte na NGC:
```
https://ngc.nvidia.com/
```

### B) P≈ôihlaste se nebo vytvo≈ôte √∫ƒçet
- Pou≈æijte firemn√≠ nebo osobn√≠ email
- NVIDIA √∫ƒçet je zdarma

### C) Vygenerujte API Key:
1. P≈ôejdƒõte na: https://ngc.nvidia.com/setup/api-key
2. Kliknƒõte na **"Generate API Key"**
3. Zkop√≠rujte kl√≠ƒç (86 znak≈Ø, konƒç√≠ na `==`)

### D) Nastavte jako environment variable:

#### Doƒçasnƒõ (pro aktu√°ln√≠ session):
```bash
export NGC_API_KEY=<your_api_key_here>
```

#### Trvale (pro v≈°echny session):
```bash
echo 'export NGC_API_KEY=<your_api_key_here>' >> ~/.bashrc
source ~/.bashrc
```

### E) Ovƒõ≈ôte nastaven√≠:
```bash
echo $NGC_API_KEY | grep -E '^[a-zA-Z0-9]{86}=='
```

Mƒõli byste vidƒõt: `‚úì` (checkmark) pokud je form√°t spr√°vn√Ω.

---

## üöÄ Krok 2: Instalace NIM

### Metoda 1: Automatick√Ω script (doporuƒçeno)

```bash
# Spustit instalaƒçn√≠ script
~/setup_nim.sh
```

Script v√°s provede:
1. Ovƒõ≈ôen√≠m NGC API key
2. P≈ôihl√°≈°en√≠m do NGC registry
3. V√Ωbƒõrem modelu (Llama 3.1 8B nebo Qwen3-32B)
4. Sta≈æen√≠m a spu≈°tƒõn√≠m NIM containeru

### Metoda 2: Manu√°ln√≠ instalace

#### Krok 2.1: P≈ôihl√°≈°en√≠ do NGC
```bash
echo $NGC_API_KEY | docker login nvcr.io -u '$oauthtoken' --password-stdin
```

#### Krok 2.2: Sta≈æen√≠ NIM containeru

**Pro Llama 3.1 8B:**
```bash
docker pull nvcr.io/nim/meta/llama-3.1-8b-instruct:1.2.4
```

**Pro Qwen3-32B (Spark-specific):**
```bash
docker pull nvcr.io/nim/qwen/qwen3-32b-dgx-spark:latest
```

#### Krok 2.3: Vytvo≈ôen√≠ cache adres√°≈ôe
```bash
mkdir -p ~/.cache/nim
```

#### Krok 2.4: Spu≈°tƒõn√≠ NIM

**Pro Llama 3.1 8B:**
```bash
docker run -d \
  --name nim-llama31-8b \
  --gpus all \
  --shm-size=16GB \
  -e NGC_API_KEY \
  -v ~/.cache/nim:/opt/nim/.cache \
  -p 8000:8000 \
  nvcr.io/nim/meta/llama-3.1-8b-instruct:1.2.4
```

**Pro Qwen3-32B:**
```bash
docker run -d \
  --name nim-qwen3-32b \
  --gpus all \
  --shm-size=16GB \
  -e NGC_API_KEY \
  -v ~/.cache/nim:/opt/nim/.cache \
  -p 8000:8000 \
  nvcr.io/nim/qwen/qwen3-32b-dgx-spark:latest
```

---

## üß™ Krok 3: Testov√°n√≠ NIM

### A) Zkontrolovat, ≈æe NIM bƒõ≈æ√≠:
```bash
docker ps | grep nim
```

Mƒõli byste vidƒõt bƒõ≈æ√≠c√≠ container.

### B) Sledovat logy (ƒçek√°me na "Server started"):
```bash
docker logs -f nim-llama31-8b   # nebo nim-qwen3-32b
```

Poƒçkejte, dokud neuvid√≠te:
```
‚úì Server started on 0.0.0.0:8000
```

### C) Test API - z√°kladn√≠ completion:

**Llama 3.1 8B:**
```bash
curl http://localhost:8000/v1/completions \
  -H "Content-Type: application/json" \
  -d '{
    "model": "meta/llama-3.1-8b-instruct",
    "prompt": "Once upon a time",
    "max_tokens": 64,
    "temperature": 0.7
  }'
```

**Qwen3-32B:**
```bash
curl http://localhost:8000/v1/completions \
  -H "Content-Type: application/json" \
  -d '{
    "model": "qwen/qwen3-32b",
    "prompt": "Once upon a time",
    "max_tokens": 64,
    "temperature": 0.7
  }'
```

### D) Test API - chat completion:

```bash
curl http://localhost:8000/v1/chat/completions \
  -H "Content-Type: application/json" \
  -d '{
    "model": "meta/llama-3.1-8b-instruct",
    "messages": [
      {"role": "user", "content": "What is the capital of France?"}
    ],
    "max_tokens": 100
  }'
```

### E) OpenAI Python klient:

```python
from openai import OpenAI

client = OpenAI(
    base_url="http://localhost:8000/v1",
    api_key="not-used"  # NIM nepot≈ôebuje API key pro lok√°ln√≠ pou≈æit√≠
)

response = client.chat.completions.create(
    model="meta/llama-3.1-8b-instruct",
    messages=[
        {"role": "user", "content": "Hello, who are you?"}
    ],
    max_tokens=100
)

print(response.choices[0].message.content)
```

---

## üìä Monitoring a spr√°va

### Zobrazit bƒõ≈æ√≠c√≠ NIM:
```bash
docker ps | grep nim
```

### Zobrazit logy:
```bash
docker logs -f nim-llama31-8b   # nebo nim-qwen3-32b
```

### Zkontrolovat GPU vyu≈æit√≠:
```bash
nvidia-smi
```

### Zkontrolovat vyu≈æit√≠ pamƒõti:
```bash
docker stats nim-llama31-8b   # nebo nim-qwen3-32b
```

### Zastavit NIM:
```bash
docker stop nim-llama31-8b   # nebo nim-qwen3-32b
```

### Spustit znovu:
```bash
docker start nim-llama31-8b   # nebo nim-qwen3-32b
```

### Restartovat NIM:
```bash
docker restart nim-llama31-8b   # nebo nim-qwen3-32b
```

### Odstranit container:
```bash
docker stop nim-llama31-8b
docker rm nim-llama31-8b
```

### Vymazat cache (uvoln√≠ disk space):
```bash
rm -rf ~/.cache/nim
```

---

## üîß Troubleshooting

### Probl√©m: NIM se nespust√≠

**≈òe≈°en√≠ 1:** Zkontrolujte NGC API key
```bash
echo $NGC_API_KEY
```

**≈òe≈°en√≠ 2:** Zkontrolujte Docker a GPU
```bash
docker run --rm --gpus all nvcr.io/nvidia/cuda:13.0.1-devel-ubuntu24.04 nvidia-smi
```

**≈òe≈°en√≠ 3:** Zkontrolujte logy
```bash
docker logs nim-llama31-8b
```

### Probl√©m: Port 8000 ji≈æ pou≈æ√≠v√°n

**≈òe≈°en√≠:** Pou≈æijte jin√Ω port
```bash
# Zastavte jin√Ω service na portu 8000, nebo zmƒõ≈àte port:
docker run -d \
  --name nim-llama31-8b \
  --gpus all \
  --shm-size=16GB \
  -e NGC_API_KEY \
  -v ~/.cache/nim:/opt/nim/.cache \
  -p 8001:8000 \
  nvcr.io/nim/meta/llama-3.1-8b-instruct:1.2.4

# Pak pou≈æijte: http://localhost:8001/v1
```

### Probl√©m: Out of memory

**≈òe≈°en√≠:** Pou≈æijte men≈°√≠ model nebo zvy≈°te swap
```bash
# Zkontrolujte pamƒõ≈•
free -h

# Llama 3.1 8B pot≈ôebuje ~16GB
# Qwen3-32B pot≈ôebuje ~40GB

# Pou≈æijte Llama 3.1 8B m√≠sto Qwen3-32B
```

### Probl√©m: Slow inference

**≈òe≈°en√≠ 1:** Zkontrolujte GPU utilization
```bash
nvidia-smi -l 1
```

**≈òe≈°en√≠ 2:** Zkontrolujte, ≈æe nebƒõ≈æ√≠ jin√© GPU procesy
```bash
ps aux | grep -E "python|ollama|lmstudio"
```

---

## üìñ Dokumentace a resources

### Ofici√°ln√≠ dokumentace:
- **NIM Release Notes:** https://docs.nvidia.com/nim/large-language-models/1.14.0/release-notes.html
- **NGC Catalog:** https://catalog.ngc.nvidia.com/
- **DGX Spark NGC Guide:** https://docs.nvidia.com/dgx/dgx-spark/ngc.html

### API dokumentace:
- **OpenAI API Spec:** https://platform.openai.com/docs/api-reference
- NIM je kompatibiln√≠ s OpenAI API

### Dal≈°√≠ NIM modely:
Prohl√©dnƒõte si NGC Catalog pro v√≠ce model≈Ø:
```bash
# P≈ôihlaste se a prohledejte:
https://catalog.ngc.nvidia.com/orgs/nim
```

---

## üí° Tipy a triky

### 1. Pou≈æit√≠ s Ollama a LM Studio souƒçasnƒõ
NIM pou≈æ√≠v√° port 8000, Ollama 11434, LM Studio 41343. M≈Ø≈æete je v≈°echny provozovat z√°rove≈à!

### 2. Benchmark performance
```bash
# Nainstalujte vegeta (HTTP load testing)
go install github.com/tsenart/vegeta@latest

# Test throughput
echo "GET http://localhost:8000/v1/models" | vegeta attack -duration=10s -rate=10 | vegeta report
```

### 3. Persistent storage
Cache v `~/.cache/nim` obsahuje sta≈æen√© modely. Nesma≈æte, pokud nechcete znovu stahovat!

### 4. Multiple NIM instances
M≈Ø≈æete bƒõ≈æet v√≠ce NIM souƒçasnƒõ na r≈Øzn√Ωch portech:
```bash
# Llama 3.1 8B na portu 8000
docker run -d --name nim-llama-8000 -p 8000:8000 ...

# Qwen3-32B na portu 8001
docker run -d --name nim-qwen-8001 -p 8001:8000 ...
```

---

## üéØ Use Cases

### 1. Lok√°ln√≠ development
Pou≈æ√≠vejte NIM m√≠sto OpenAI API pro testov√°n√≠ bez cloudu:
```python
# Zmƒõ≈àte pouze base_url
client = OpenAI(base_url="http://localhost:8000/v1", api_key="dummy")
```

### 2. Production inference
NIM je optimalizovan√Ω pro high-throughput inference v produkci.

### 3. Fine-tuned models
M≈Ø≈æete nasadit svoje fine-tuned modely p≈ôes NIM (vy≈æaduje custom build).

---

**M√°te NIM p≈ôipraven√Ω k pou≈æit√≠!** üéâ

Pro ot√°zky nebo probl√©my:
- ƒåtƒõte dokumentaci: `/home/puzik/START_NIM.md`
- Pou≈æijte instalaƒçn√≠ script: `~/setup_nim.sh`
- Pod√≠vejte se na kompletn√≠ souhrn: `~/KOMPLETNI_SOUHRN_2025-11-14.md`
