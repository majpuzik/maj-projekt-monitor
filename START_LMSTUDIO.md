# RychlÃ½ start LM Studio

**LM Studio verze:** 0.3.31 (ARM64)
**SystÃ©m:** DGX Spark GB10
**SpeciÃ¡lnÃ­ optimalizace:** CUDA 13.0 pro DGX Spark

---

## ğŸš€ SpuÅ¡tÄ›nÃ­ LM Studio

### 1. ZÃ¡kladnÃ­ spuÅ¡tÄ›nÃ­ GUI
```bash
~/LMStudio-0.3.31-arm64.AppImage
```

### 2. VytvoÅ™enÃ­ symlinku (pro pohodlÃ­)
```bash
sudo ln -s ~/LMStudio-0.3.31-arm64.AppImage /usr/local/bin/lms
# Pak mÅ¯Å¾ete spustit jen:
lms
```

---

## ğŸ“¥ StaÅ¾enÃ­ prvnÃ­ch modelÅ¯

### DoporuÄenÃ© modely pro DGX Spark (128GB RAM):

#### **Pro rychlÃ© testovÃ¡nÃ­:**
- **Qwen/Qwen2.5-7B** (7B parametrÅ¯, ~4-8 GB)
- **mistralai/Mistral-7B-v0.1** (7B parametrÅ¯, ~4-8 GB)

#### **Pro pokroÄilou prÃ¡ci:**
- **meta-llama/Llama-3.2-8B** (8B parametrÅ¯, ~5-10 GB)
- **Qwen/Qwen3-Coder-14B** (14B parametrÅ¯, ~8-16 GB)

#### **VelkÃ© modely (vyuÅ¾ijete 128GB RAM!):**
- **meta-llama/Llama-3-70B** (70B parametrÅ¯, ~40-80 GB)
- **Qwen/Qwen2.5-72B** (72B parametrÅ¯, ~40-80 GB)

### V GUI:
1. Kliknout na "ğŸ” Discover" nebo "ğŸ“¥ Download models"
2. Vyhledat model (napÅ™. "Qwen 7B")
3. Vybrat kvantizaci:
   - **Q4_K_M** - dobrÃ½ pomÄ›r velikost/kvalita (doporuÄeno)
   - **Q5_K_M** - lepÅ¡Ã­ kvalita, vÄ›tÅ¡Ã­ soubor
   - **Q8_0** - tÃ©mÄ›Å™ plnÃ¡ kvalita
4. Kliknout na "Download"

---

## ğŸŒ SpuÅ¡tÄ›nÃ­ jako LLM Server

### V GUI:
1. NaÄÃ­st model (klik na model â†’ "Load")
2. PÅ™ejÃ­t na "Developer" tab
3. ZaÅ¡krtnout "âœ… Serve on Local Network"
4. Server pobÄ›Å¾Ã­ na `http://localhost:1234`

### Test serveru:
```bash
# OvÄ›Å™it, Å¾e server bÄ›Å¾Ã­
curl http://localhost:1234/v1/models

# Test chat completion
curl http://localhost:1234/v1/chat/completions \
  -H "Content-Type: application/json" \
  -d '{
    "model": "local-model",
    "messages": [{"role": "user", "content": "Hello!"}]
  }'
```

---

## ğŸ”— OpenAI-kompatibilnÃ­ API

LM Studio poskytuje OpenAI-kompatibilnÃ­ API endpoint:

### Python pÅ™Ã­klad:
```python
from openai import OpenAI

# PÅ™ipojit se k LM Studio serveru
client = OpenAI(
    base_url="http://localhost:1234/v1",
    api_key="not-needed"
)

# Chat completion
response = client.chat.completions.create(
    model="local-model",
    messages=[
        {"role": "system", "content": "You are a helpful assistant."},
        {"role": "user", "content": "Explain quantum computing in simple terms."}
    ]
)

print(response.choices[0].message.content)
```

### Node.js/JavaScript pÅ™Ã­klad:
```javascript
const OpenAI = require('openai');

const client = new OpenAI({
  baseURL: 'http://localhost:1234/v1',
  apiKey: 'not-needed'
});

const response = await client.chat.completions.create({
  model: 'local-model',
  messages: [
    { role: 'user', content: 'Hello!' }
  ]
});

console.log(response.choices[0].message.content);
```

---

## ğŸ“¦ Import vlastnÃ­ch GGUF modelÅ¯

Pokud mÃ¡te GGUF soubory z jinÃ½ch zdrojÅ¯:

1. **V GUI:** Model â†’ Import â†’ vybrat .gguf soubor
2. **Nebo zkopÃ­rovat do:** `~/.cache/lm-studio/models/`

### SdÃ­lenÃ­ modelÅ¯ mezi Ollama a LM Studio:

**Z Ollama do LM Studio:**
```bash
# NajÃ­t Ollama model
ls ~/.ollama/models/blobs/

# Import do LM Studio (v GUI): Model â†’ Import
```

**Z LM Studio do Ollama:**
```bash
# NajÃ­t LM Studio model
ls ~/.cache/lm-studio/models/

# VytvoÅ™it Modelfile pro Ollama
cat > Modelfile <<EOF
FROM /cesta/k/modelu.gguf
PARAMETER temperature 0.7
EOF

# VytvoÅ™it Ollama model
ollama create muj-model -f Modelfile
```

---

## âš™ï¸ PokroÄilÃ© nastavenÃ­

### GPU Acceleration (CUDA)
LM Studio automaticky detekuje NVIDIA GPU a pouÅ¾Ã­vÃ¡ CUDA 13.0.

**OvÄ›Å™enÃ­ GPU:**
- V GUI: Settings â†’ Hardware
- MÄ›li byste vidÄ›t: "NVIDIA GB10"

### Ãšspora pamÄ›ti
Pokud narazÃ­te na problÃ©my s pamÄ›tÃ­ (Unified Memory Architecture):
```bash
# Mimo LM Studio - vyÄistit cache
sudo sh -c 'sync; echo 3 > /proc/sys/vm/drop_caches'
```

### Remote Access
Pro pÅ™Ã­stup z jinÃ½ch zaÅ™Ã­zenÃ­ v sÃ­ti:
1. GUI â†’ Developer â†’ "Serve on Local Network"
2. Server bude pÅ™Ã­stupnÃ½ na: `http://<IP-adresa-sparku>:1234`

Zjistit IP adresu:
```bash
hostname -I | awk '{print $1}'
```

---

## ğŸ†š LM Studio vs Ollama

| Vlastnost | Ollama | LM Studio |
|-----------|--------|-----------|
| **Interface** | CLI | GUI + CLI |
| **Port** | 11434 | 1234 |
| **Model browser** | âŒ | âœ… (prohlÃ­Å¾et HuggingFace) |
| **API** | REST | OpenAI-compatible |
| **StejnÃ½ engine** | âœ… llama.cpp | âœ… llama.cpp |
| **GGUF soubory** | âœ… | âœ… |

**MÅ¯Å¾ete provozovat OBA souÄasnÄ›!**

---

## ğŸ› Troubleshooting

### GUI se nespustÃ­
```bash
# Zkontrolovat, zda je soubor executable
chmod +x ~/LMStudio-0.3.31-arm64.AppImage

# Zkontrolovat zÃ¡vislosti
ldd ~/LMStudio-0.3.31-arm64.AppImage
```

### Model se nenaÄte
- Zkontrolovat volnou pamÄ›Å¥: `free -h`
- Vybrat menÅ¡Ã­ kvantizaci (Q4_K_M mÃ­sto Q8_0)

### Server nereaguje
```bash
# Zkontrolovat, zda server bÄ›Å¾Ã­
curl http://localhost:1234/v1/models

# Zkontrolovat port
netstat -tulpn | grep 1234
```

---

## ğŸ“š DalÅ¡Ã­ zdroje

- **OficiÃ¡lnÃ­ docs:** https://lmstudio.ai/docs/app
- **DGX Spark blog post:** https://lmstudio.ai/blog/dgx-spark
- **GitHub issues:** https://github.com/lmstudio-ai/lmstudio-bug-tracker
- **Discord:** https://discord.gg/lmstudio

---

**UÅ¾ijte si lokÃ¡lnÃ­ LLM na DGX Spark!** ğŸš€
